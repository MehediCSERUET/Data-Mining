{"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[]},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Sigmoid scoring on fused features, and concatenate attention output with scored fused features as input to the MLP classifier. 85.29% overall AUC","metadata":{"id":"XSGXueiORJ0P"}},{"cell_type":"code","source":"# combined_max_accuracy_attempt_no_validation_metrics.py\n# Modified to:\n# 1. Use sigmoid scoring on fused features, and concatenate attention output\n#    with scored fused features as input to the MLP classifier.\n# 2. Include comprehensive metrics: Overall AUC, Accuracy, F1, Sensitivity, Specificity.\n# 3. Group results by Overall, Asian, Black, White, Male, Female.\n\nimport os, sys, zipfile, glob, subprocess, copy, math, time, traceback\nfrom typing import Tuple, Dict, Any\n\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport torchvision.models as models\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n\nimport matplotlib.pyplot as plt\n\n# -------------------------\n# User-editable config (tune these)\n# -------------------------\nGDRIVE_LINK = \"Drive link to the fairdomain zip file\"\nFAIRDOMAIN_ZIP = \"FairDomain.zip\"\nEXTRACT_ROOT = \"dataset_extracted\"\nTRAIN_DIR = os.path.join(EXTRACT_ROOT, \"Training\")\nTEST_DIR  = os.path.join(EXTRACT_ROOT, \"Testing\")\nREPO_DIR  = \"/content/trustworthyAI\"\nSUMMARY_CSV_PATH = \"/content/data_summary.csv\"  # Added for demographic lookups\n\n# Model/training hyperparameters\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nSEED = 42\nBATCH_SIZE = 24                      # reduce if OOM; use 16 if needed\nNUM_EPOCHS_STAGE1 = 10\nNUM_EPOCHS_STAGE2 = 50\nHEAD_LR = 1e-3\nBACKBONE_LR = 3e-5\nWEIGHT_DECAY = 1e-5\nINPUT_MEAN = 0.5\nINPUT_STD  = 0.25\n\n# Latent sizes (you asked 64x64 adjacency earlier; left here as originally set)\nZ1_DIM = 64\nZ2_DIM = 4\nZ_DIM = Z1_DIM * Z2_DIM\n\n# backbone/proj sizes\nBACKBONE = \"efficientnet_b3\"   # options: efficientnet_b3, efficientnet_b0, densenet121, resnet34\nPROJ_DIM = 1024\n\nGLAUCOMA_MLP_HIDDEN = 256\nMAX_GRAD_NORM = 1.0\nKL_ANNEAL_EPOCHS = 8\nEARLY_STOP_PATIENCE = 12\n\nUSE_MIXUP = True\nMIXUP_ALPHA = 0.2\nUSE_SWA = True\nSWA_START_EPOCH = max(1, NUM_EPOCHS_STAGE2 - 5)  # start SWA near the end\n# -------------------------\n\ndef run_shell(cmd):\n    print(\"RUN:\", cmd)\n    r = subprocess.run(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    if r.returncode != 0:\n        print(r.stderr.decode(\"utf-8\"))\n    return r\n\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\n# download + extract FairDomain.zip and inner dataset.zip\ndef download_and_extract_fairdomain(gdrive_link=GDRIVE_LINK, out_zip=FAIRDOMAIN_ZIP):\n    try:\n        import gdown\n    except Exception:\n        print(\"Installing gdown...\")\n        run_shell(\"pip install -q gdown\")\n        import gdown\n    # parse id\n    parts = gdrive_link.split(\"/\")\n    file_id = None\n    for i,p in enumerate(parts):\n        if p == \"d\" and i+1 < len(parts):\n            file_id = parts[i+1]; break\n    if file_id is None:\n        file_id = gdrive_link.split(\"/\")[-2]\n    print(\"Downloading FairDomain.zip (id=%s)...\" % file_id)\n    gdown.download(id=file_id, output=out_zip, quiet=False)\n    if not os.path.exists(out_zip):\n        raise RuntimeError(\"Download failed: %s\" % out_zip)\n    print(\"Extracting FairDomain.zip ...\")\n    with zipfile.ZipFile(out_zip, \"r\") as z:\n        z.extractall(\".\")\n    # find inner dataset zip\n    candidates = glob.glob(\"**/dataset*.zip\", recursive=True)\n    if not candidates:\n        candidates = glob.glob(\"**/dataset.zip\", recursive=True)\n    if not candidates:\n        raise RuntimeError(\"Could not find dataset.zip inside FairDomain.zip extracted contents.\")\n    dataset_zip = candidates[0]\n    print(\"Found dataset:\", dataset_zip)\n    os.makedirs(EXTRACT_ROOT, exist_ok=True)\n    with zipfile.ZipFile(dataset_zip, \"r\") as z:\n        z.extractall(EXTRACT_ROOT)\n    print(\"Dataset extracted to\", EXTRACT_ROOT)\n\ndef clone_trustworthyai(repo_dir=REPO_DIR):\n    if not os.path.exists(repo_dir):\n        print(\"Cloning trustworthyAI...\")\n        run_shell(f\"git clone https://github.com/huawei-noah/trustworthyAI.git {repo_dir}\")\n    else:\n        print(\"trustworthyAI already present at\", repo_dir)\n    sys.path.insert(0, os.path.join(repo_dir, \"research\", \"CausalVAE\", \"codebase\"))\n    sys.path.insert(0, os.path.join(repo_dir, \"research\", \"CausalVAE\", \"codebase\", \"models\", \"nns\"))\n    sys.path.insert(0, os.path.join(repo_dir, \"research\", \"CausalVAE\"))\n\ndef normalize_race_value(rv) -> int:\n    \"\"\" Maps race inputs to integers: 0=White, 1=Black, 2=Asian, 3=Other \"\"\"\n    if rv is None: return 3\n    try:\n        if isinstance(rv, (str, bytes, bytearray)):\n             s = str(rv).strip().lower()\n             if 'white' in s: return 0\n             if 'black' in s or 'african' in s: return 1\n             if 'asian' in s: return 2\n             return 3\n        if np.isscalar(rv):\n            if int(rv) in (0,1,2,3): return int(rv)\n            return 3\n        return 3\n    except Exception:\n        return 3\n\ndef load_summary_csv_as_map(csv_path: str) -> Dict[str, Dict[str, Any]]:\n    if not os.path.exists(csv_path):\n        print(f\"Warning: Summary CSV not found at {csv_path}\")\n        return {}\n    try:\n        df = pd.read_csv(csv_path)\n    except Exception as e:\n        print(f\"Error reading CSV: {e}\")\n        return {}\n\n    possible_id_cols = [c for c in df.columns if c.lower() in ('id','subject_id','patient_id','filename','file','case','name')]\n    chosen_id_col = possible_id_cols[0] if possible_id_cols else None\n\n    if chosen_id_col is None: return {}\n\n    df['_key'] = df[chosen_id_col].astype(str).str.strip()\n    mapping = {}\n\n    for _, row in df.iterrows():\n        key = row['_key']\n        key_clean = os.path.splitext(key)[0]\n        mapping[key] = row.to_dict()\n        mapping[key_clean] = row.to_dict()\n\n    return mapping\n\ndef load_npz_dir(root_dir):\n    out = {}\n    if not os.path.exists(root_dir):\n        print(\"Warning: path does not exist:\", root_dir)\n        return out\n    for root, _, files in os.walk(root_dir):\n        for f in files:\n            if f.endswith(\".npz\"):\n                p = os.path.join(root, f)\n                k = os.path.splitext(f)[0]\n                try:\n                    data = np.load(p, allow_pickle=True)\n                    out[k] = dict(data)\n                except Exception as e:\n                    print(\"Failed to load\", p, e)\n    return out\n\ndef process_dataset_data_to_arrays(data_dict, summary_map: Dict[str, Dict[str, Any]] = None):\n    octs, slos, labels = [], [], []\n    keys = sorted(data_dict.keys())\n    for k in keys:\n        d = data_dict[k]\n        if \"oct_fundus\" not in d or \"slo_fundus\" not in d:\n            continue\n        o = d[\"oct_fundus\"]; s = d[\"slo_fundus\"]\n        if o.ndim == 3 and o.shape[-1] == 1: o = np.squeeze(o, axis=-1)\n        if s.ndim == 3 and s.shape[-1] == 1: s = np.squeeze(s, axis=-1)\n\n        # Defaults\n        glaucoma = float(d.get(\"glaucoma\", 0.0))\n        gender = float(d.get(\"gender\", 0.0))\n        age = float(d.get(\"age\", 0.0))\n        md = float(d.get(\"md\", 0.0))\n        race_val = d.get(\"race\", None)\n\n        # Override/Augment from CSV if available\n        if summary_map and k in summary_map:\n            entry = summary_map[k]\n            if 'glaucoma' in entry and pd.notna(entry['glaucoma']):\n                g_val = str(entry['glaucoma']).lower()\n                glaucoma = 1.0 if g_val in ['yes', '1', 'true'] else 0.0\n            if 'gender' in entry and pd.notna(entry['gender']):\n                g_val = str(entry['gender']).lower()\n                # Assuming 1=Female, 0=Male standard; adjust if dataset differs\n                gender = 1.0 if g_val in ['female', 'f', '1'] else 0.0\n            if 'race' in entry and pd.notna(entry['race']):\n                race_val = entry['race']\n            if 'age' in entry and pd.notna(entry['age']):\n                try: age = float(entry['age'])\n                except: pass\n            if 'md' in entry and pd.notna(entry['md']):\n                try: md = float(entry['md'])\n                except: pass\n\n        race_code = normalize_race_value(race_val)\n\n        octs.append(o.astype(np.float32)); slos.append(s.astype(np.float32))\n        labels.append([glaucoma, gender, age, md, float(race_code)])\n\n    if len(octs) == 0: return np.array([]), np.array([]), np.array([])\n    return np.array(octs), np.array(slos), np.array(labels, dtype=np.float32)\n\nclass DualImageDataset(Dataset):\n    def __init__(self, oct_images, slo_images, labels, transform=None):\n        assert len(oct_images) == len(slo_images) == len(labels)\n        self.oct_images = oct_images; self.slo_images = slo_images; self.labels = labels; self.transform = transform\n    def _to_pil(self,a):\n        arr = a\n        if arr.dtype != np.uint8:\n            if arr.max() <= 1.0:\n                arr = (arr*255.0).astype(np.uint8)\n            elif arr.max() <= 255.0 and arr.min() >= 0:\n                arr = arr.astype(np.uint8)\n            else:\n                amin = float(arr.min()); amax = float(arr.max())\n                if amax-amin > 1e-8:\n                    arr = ((arr-amin)/(amax-amin)*255.0).astype(np.uint8)\n                else:\n                    arr = np.zeros_like(arr, dtype=np.uint8)\n        return Image.fromarray(arr)\n    def __len__(self): return len(self.oct_images)\n    def __getitem__(self, idx):\n        o = self.oct_images[idx]; s = self.slo_images[idx]\n        o_pil = self._to_pil(o); s_pil = self._to_pil(s)\n        if self.transform:\n            o_t = self.transform(o_pil); s_t = self.transform(s_pil)\n        else:\n            o_t = transforms.ToTensor()(o_pil); s_t = transforms.ToTensor()(s_pil)\n        lbl = torch.tensor(self.labels[idx], dtype=torch.float32)\n        return o_t, s_t, lbl\n\n# -------------------------\n# Model building blocks (kept from your file)\n# -------------------------\ndef gaussian_parameters(h, dim=-1):\n    m, v = torch.split(h, h.size(dim)//2, dim=dim)\n    v = F.softplus(v) + 1e-8\n    return m, v\n\ndef build_backbone(backbone='efficientnet_b3', pretrained=True):\n    b = backbone.lower()\n    if \"efficientnet_b3\" in b:\n        try:\n            from torchvision.models import EfficientNet_B3_Weights\n            weights = EfficientNet_B3_Weights.IMAGENET1K_V1 if pretrained else None\n            net = models.efficientnet_b3(weights=weights)\n        except Exception:\n            net = models.efficientnet_b3(pretrained=bool(pretrained))\n        try:\n            feat_dim = net.classifier[1].in_features\n        except Exception:\n            feat_dim = 1536\n        features_extractor = net.features\n        return net, feat_dim, features_extractor\n    elif \"efficientnet_b0\" in b:\n        try:\n            from torchvision.models import EfficientNet_B0_Weights\n            weights = EfficientNet_B0_Weights.IMAGENET1K_V1 if pretrained else None\n            net = models.efficientnet_b0(weights=weights)\n        except Exception:\n            net = models.efficientnet_b0(pretrained=bool(pretrained))\n        try:\n            feat_dim = net.classifier[1].in_features\n        except Exception:\n            feat_dim = 1280\n        features_extractor = net.features\n        return net, feat_dim, features_extractor\n    elif \"densenet121\" in b:\n        try:\n            from torchvision.models import DenseNet121_Weights\n            weights = DenseNet121_Weights.IMAGENET1K_V1 if pretrained else None\n            net = models.densenet121(weights=weights)\n        except Exception:\n            net = models.densenet121(pretrained=bool(pretrained))\n        feat_dim = 1024\n        features_extractor = nn.Sequential(net.features, nn.ReLU(inplace=True))\n        return net, feat_dim, features_extractor\n    else:\n        try:\n            from torchvision.models import ResNet34_Weights\n            weights = ResNet34_Weights.IMAGENET1K_V1 if pretrained else None\n            net = models.resnet34(weights=weights)\n        except Exception:\n            net = models.resnet34(pretrained=bool(pretrained))\n        feat_dim = 512\n        features_extractor = nn.Sequential(*list(net.children())[:-2])\n        return net, feat_dim, features_extractor\n\nclass SingleEncoderGeneric(nn.Module):\n    def __init__(self, backbone='efficientnet_b3', pretrained=True):\n        super().__init__()\n        net, feat_dim, extractor = build_backbone(backbone=backbone, pretrained=pretrained)\n        self.backbone_name = backbone.lower()\n        if hasattr(net, 'features') and self.backbone_name.startswith('efficientnet'):\n            try:\n                first_block = net.features[0]\n                replaced = False\n                for i,m in enumerate(first_block):\n                    if isinstance(m, nn.Conv2d):\n                        old_conv = m\n                        new_conv = nn.Conv2d(1, old_conv.out_channels, kernel_size=old_conv.kernel_size,\n                                             stride=old_conv.stride, padding=old_conv.padding, bias=(old_conv.bias is not None))\n                        try:\n                            with torch.no_grad():\n                                old_w = old_conv.weight.data\n                                new_w = old_w.mean(dim=1, keepdim=True)\n                                new_conv.weight.copy_(new_w)\n                        except Exception:\n                            pass\n                        first_block[i] = new_conv\n                        replaced = True\n                        break\n                if not replaced:\n                    old_conv = net.features[0][0]\n                    new_conv = nn.Conv2d(1, old_conv.out_channels, kernel_size=old_conv.kernel_size,\n                                         stride=old_conv.stride, padding=old_conv.padding, bias=(old_conv.bias is not None))\n                    try:\n                        with torch.no_grad():\n                            old_w = old_conv.weight.data\n                            new_w = old_w.mean(dim=1, keepdim=True)\n                            new_conv.weight.copy_(new_w)\n                    except Exception:\n                        pass\n                    net.features[0][0] = new_conv\n            except Exception:\n                pass\n            self.backbone = net.features\n        elif hasattr(net, 'features') and self.backbone_name.startswith('densenet'):\n            try:\n                old_conv = net.features.conv0\n                new_conv = nn.Conv2d(1, old_conv.out_channels, kernel_size=old_conv.kernel_size,\n                                     stride=old_conv.stride, padding=old_conv.padding, bias=(old_conv.bias is not None))\n                try:\n                    with torch.no_grad():\n                        old_w = old_conv.weight.data\n                        new_w = old_w.mean(dim=1, keepdim=True)\n                        new_conv.weight.copy_(new_w)\n                except Exception:\n                    pass\n                net.features.conv0 = new_conv\n            except Exception:\n                pass\n            self.backbone = nn.Sequential(net.features, nn.ReLU(inplace=True))\n        else:\n            try:\n                old_conv = net.conv1\n                new_conv = nn.Conv2d(1, old_conv.out_channels, kernel_size=old_conv.kernel_size,\n                                     stride=old_conv.stride, padding=old_conv.padding, bias=(old_conv.bias is not None))\n                try:\n                    with torch.no_grad():\n                        old_w = old_conv.weight.data\n                        new_w = old_w.mean(dim=1, keepdim=True)\n                        new_conv.weight.copy_(new_w)\n                except Exception:\n                    pass\n                net.conv1 = new_conv\n            except Exception:\n                pass\n            self.backbone = extractor\n        self.pool = nn.AdaptiveAvgPool2d((1,1))\n        self.feat_dim = feat_dim\n\n    def forward(self, x):\n        if x.dim() == 3: x = x.unsqueeze(1)\n        f = self.backbone(x)\n        if f.dim() == 4:\n            f = self.pool(f).view(f.size(0), -1)\n        elif f.dim() == 2:\n            pass\n        else:\n            f = f.view(f.size(0), -1)\n        return f\n\nclass DualSeparateEncoders(nn.Module):\n    def __init__(self, z_dim:int, z1_dim:int, z2_dim:int, backbone='efficientnet_b3', pretrained=True, proj_dim=PROJ_DIM):\n        super().__init__()\n        self.z_dim = z_dim; self.z1_dim = z1_dim; self.z2_dim = z2_dim\n        self.enc_o = SingleEncoderGeneric(backbone=backbone, pretrained=pretrained)\n        self.enc_s = SingleEncoderGeneric(backbone=backbone, pretrained=pretrained)\n        feat_dim = self.enc_o.feat_dim\n        self.proj_dim = proj_dim\n        self.proj_o = nn.Sequential(nn.Linear(feat_dim, proj_dim), nn.ReLU())\n        self.proj_s = nn.Sequential(nn.Linear(feat_dim, proj_dim), nn.ReLU())\n        self.gate = nn.Sequential(\n            nn.Linear(proj_dim*2, proj_dim//2),\n            nn.ReLU(),\n            nn.Linear(proj_dim//2, proj_dim),\n            nn.Sigmoid()\n        )\n        self.fusion_out = nn.Sequential(\n            nn.Linear(proj_dim, 1024), nn.ReLU(), nn.Dropout(0.2), nn.Linear(1024, 2*z_dim)\n        )\n    def encode(self, oct_img, slo_img) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        fo = self.enc_o(oct_img); fs = self.enc_s(slo_img)\n        fo_p = self.proj_o(fo); fs_p = self.proj_s(fs)\n        g_in = torch.cat([fo_p, fs_p], dim=1)\n        gate = self.gate(g_in)\n        fused = gate * fo_p + (1.0 - gate) * fs_p\n        h = self.fusion_out(fused)\n        m, v = gaussian_parameters(h, dim=1)\n        return m, v, fused\n\nclass GlaucomaPredictorMLP(nn.Module):\n    def __init__(self, input_dim, hidden_dim=GLAUCOMA_MLP_HIDDEN, dropout=0.4):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.BatchNorm1d(hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, max(8, hidden_dim//2)),\n            nn.BatchNorm1d(max(8, hidden_dim//2)),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout/2),\n            nn.Linear(max(8, hidden_dim//2), 1)\n        )\n    def forward(self,x): return self.net(x)\n\nclass SimpleDecoder(nn.Module):\n    def __init__(self, z_dim, img_size=(96,96), channel=1):\n        super().__init__()\n        self.z_dim = z_dim; self.img_size = img_size; self.channel = channel\n        self.output_dim = self.channel * self.img_size[0] * self.img_size[1]\n        self.net = nn.Sequential(\n            nn.Linear(z_dim, 512),\n            nn.ELU(),\n            nn.Linear(512, 1024),\n            nn.ELU(),\n            nn.Linear(1024, self.output_dim)\n        )\n    def forward(self,z):\n        return self.net(z)\n\ndef focal_bce_logits(logits, targets, alpha=0.6, gamma=2.0):\n    bce = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')\n    p = torch.sigmoid(logits)\n    pt = p * targets + (1 - p) * (1 - targets)\n    alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n    loss = alpha_t * ((1 - pt) ** gamma) * bce\n    return loss.mean()\n\nclass CausalVAE_AttnScoring_Improved(nn.Module):\n    def __init__(self, ut_module=None, z1_dim=Z1_DIM, z2_dim=Z2_DIM, proj_dim=PROJ_DIM, device=DEVICE, backbone=BACKBONE):\n        super().__init__()\n        self.ut = ut_module\n        self.z1_dim = z1_dim; self.z2_dim = z2_dim; self.z_dim = z1_dim * z2_dim\n        self.proj_dim = proj_dim; self.device = device\n        self.enc = DualSeparateEncoders(z_dim=self.z_dim, z1_dim=self.z1_dim, z2_dim=self.z2_dim,\n                                        backbone=backbone, pretrained=True, proj_dim=self.proj_dim)\n        self.decoder = SimpleDecoder(self.z_dim, img_size=(96,96), channel=1)\n        try:\n            from mask import DagLayer, Attention, MaskLayer\n            self.dag = DagLayer(self.z1_dim, self.z1_dim, i=False, initial=True)\n            self.attn_util = Attention(self.z2_dim)\n            self.mask_z = MaskLayer(self.z_dim, concept=self.z1_dim, z2_dim=self.z2_dim)\n            self.mask_u = MaskLayer(self.z1_dim, concept=self.z1_dim, z2_dim=1)\n            print(\"DAG utilities loaded.\")\n        except Exception:\n            self.dag = None; self.attn_util = None; self.mask_z = None; self.mask_u = None\n        self.adj_to_feat = nn.Sequential(\n            nn.Linear(self.z1_dim * self.z1_dim, max(512, self.proj_dim//2)),\n            nn.ReLU(),\n            nn.Linear(max(512, self.proj_dim//2), self.proj_dim)\n        )\n        # sigmoid scoring instead of softmax\n        self.sigmoid = nn.Sigmoid()\n        # classifier input now will be (proj_dim scored fused) + flattened attention (z_dim)\n        self.glaucoma_predictor = GlaucomaPredictorMLP(input_dim=(self.proj_dim + self.z_dim), hidden_dim=GLAUCOMA_MLP_HIDDEN)\n        self.pos_weight = None\n\n    def compute_adjacency_from_latent(self, q_m_reshaped: torch.Tensor):\n        x = q_m_reshaped.mean(dim=2)\n        x = (x - x.mean(dim=1, keepdim=True)) / (x.std(dim=1, keepdim=True) + 1e-6)\n        adj = x.unsqueeze(2) * x.unsqueeze(1)\n        return adj\n\n    def negative_elbo_bound(self, oct_img, slo_img, concept_labels, sample=False, lambdav=1e-3, kl_weight=1.0):\n        device = self.device\n        oct_img = oct_img.to(device); slo_img = slo_img.to(device); labels = concept_labels.to(device)\n        q_m, q_v, fused = self.enc.encode(oct_img, slo_img)\n        batch = q_m.size(0)\n        try:\n            q_m_reshaped = q_m.view(batch, self.z1_dim, self.z2_dim)\n        except Exception:\n            q_m_reshaped = q_m.contiguous().reshape(batch, self.z1_dim, self.z2_dim)\n        q_v_effective = torch.ones_like(q_m_reshaped, device=device)\n\n        if self.dag is not None:\n            try:\n                decode_m, _ = self.dag.calculate_dag(q_m_reshaped, q_v_effective)\n                adj_logits = None\n                if hasattr(self.dag, \"A\") and getattr(self.dag, \"A\") is not None:\n                    A = getattr(self.dag, \"A\")\n                    if isinstance(A, torch.Tensor):\n                        adj_logits = A.unsqueeze(0).expand(batch, -1, -1)\n                if adj_logits is None:\n                    adj_logits = self.compute_adjacency_from_latent(decode_m)\n            except Exception:\n                adj_logits = self.compute_adjacency_from_latent(q_m_reshaped)\n        else:\n            adj_logits = self.compute_adjacency_from_latent(q_m_reshaped)\n\n        adj_flat = adj_logits.view(batch, -1)\n        feat_logits = self.adj_to_feat(adj_flat)\n\n        # ---------- sigmoid scoring on feat_logits and score fused features ----------\n        scoring = self.sigmoid(feat_logits)             # (B, proj_dim)  values in (0,1)\n        fused_scored = fused * scoring                 # elementwise\n        # ---------------------------------------------------------------------------\n\n        # ---------- compute attention e_tilde (B, z1, z2) and flatten ----------\n        e_tilde = torch.zeros_like(q_m_reshaped)\n        if self.attn_util is not None:\n            try:\n                e_tilde = self.attn_util.attention(q_m_reshaped, q_m_reshaped)[0]\n            except Exception:\n                e_tilde = torch.zeros_like(q_m_reshaped)\n        e_flat = e_tilde.view(batch, -1)               # (B, z_dim)\n        # ------------------------------------------------------------------------\n\n        # ---------- build classifier input: concat scored fused and flattened attention ----------\n        classifier_input = torch.cat([fused_scored, e_flat], dim=1)   # (B, proj_dim + z_dim)\n        # ensure dtype matches classifier parameters (defensive)\n        try:\n            first_clf_param = next(self.glaucoma_predictor.parameters())\n            clf_dtype = first_clf_param.dtype\n        except StopIteration:\n            clf_dtype = classifier_input.dtype\n        if classifier_input.dtype != clf_dtype:\n            classifier_input = classifier_input.to(clf_dtype)\n        glaucoma_logits = self.glaucoma_predictor(classifier_input)\n        # ----------------------------------------------------------------------------------\n\n        # remaining VAE pieces (decoder, sampling) - unchanged\n        decode_m = q_m_reshaped.clone()\n        f_z1 = decode_m + e_tilde\n        if sample:\n            z_given_dag = f_z1\n        else:\n            z_given_dag = f_z1 + torch.sqrt(q_v_effective * lambdav) * torch.randn_like(f_z1).to(device)\n\n        z_for_decoder = z_given_dag.view(batch, -1)\n        decoded_flat = self.decoder(z_for_decoder)\n        decoded = decoded_flat.view(batch, 1, 96, 96)\n\n        oct_in = oct_img.unsqueeze(1) if oct_img.dim()==3 else oct_img\n        resized = F.interpolate(oct_in, size=(96,96), mode=\"bilinear\", align_corners=False)\n        targets = (resized * INPUT_STD) + INPUT_MEAN\n        targets = targets.clamp(0.0,1.0)\n\n        rec_loss = F.binary_cross_entropy_with_logits(decoded, targets, reduction=\"mean\")\n\n        kl = torch.zeros(1, device=device)\n        try:\n            if self.ut is not None:\n                p_m = torch.zeros_like(q_m_reshaped); p_v = torch.ones_like(q_m_reshaped)\n                kl = 0.3 * self.ut.kl_normal(q_m_reshaped.view(-1, self.z_dim).to(device),\n                                             q_v_effective.view(-1, self.z_dim).to(device),\n                                             p_m.view(-1, self.z_dim).to(device),\n                                             p_v.view(-1, self.z_dim).to(device))\n                try:\n                    cp_m, _ = self.ut.condition_prior(np.array([[20,15],[2,2],[59.5,26.5],[10.5,4.5]]), labels, self.z2_dim)\n                    cp_v_for_kl = torch.ones([batch, self.z1_dim, self.z2_dim]).to(device)\n                    for i in range(self.z1_dim):\n                        kl = kl + self.ut.kl_normal(decode_m[:,i,:].to(device), cp_v_for_kl[:,i,:].to(device), cp_m[:,i,:].to(device), cp_v_for_kl[:,i,:].to(device))\n                    kl = torch.mean(kl)\n                except Exception:\n                    kl = torch.mean(kl)\n        except Exception:\n            kl = torch.zeros(1, device=device)\n\n        glaucoma_label = labels[:,0].view(-1,1).to(device)\n        try:\n            glaucoma_loss = focal_bce_logits(glaucoma_logits, glaucoma_label, alpha=0.6, gamma=2.0)\n        except Exception:\n            glaucoma_loss = F.binary_cross_entropy_with_logits(glaucoma_logits, glaucoma_label)\n\n        nelbo = rec_loss + kl_weight * kl + glaucoma_loss\n\n        decoded_reshaped = decoded.squeeze(1)\n        # Return tuple: keep legacy outputs plus new classifier-relevant tensors\n        # (nelbo, kl, rec, decoded_reshaped, z_given_dag, glaucoma_loss, fused, scoring, fused_scored, e_flat)\n        return nelbo, kl, rec_loss, decoded_reshaped, z_given_dag, glaucoma_loss, fused, scoring, fused_scored, e_flat\n\n    def loss(self, oct_img, slo_img, concept_labels, kl_weight=1.0):\n        out = self.negative_elbo_bound(oct_img, slo_img, concept_labels, kl_weight=kl_weight)\n        nelbo, kl, rec, _, _, glaucoma_loss = out[0], out[1], out[2], out[3], out[4], out[5]\n        return nelbo, {'gen/kl':kl, 'gen/rec':rec, 'gen/glaucoma_loss':glaucoma_loss}\n\ndef mixup_data(x1, x2, y, alpha=0.2, device=DEVICE):\n    if alpha <= 0:\n        return x1, x2, y, None, None, 1.0\n    lam = np.random.beta(alpha, alpha)\n    batch_size = x1.size(0)\n    index = torch.randperm(batch_size).to(device)\n    mixed_x1 = lam * x1 + (1 - lam) * x1[index, :]\n    mixed_x2 = lam * x2 + (1 - lam) * x2[index, :]\n    y_a, y_b = y, y[index]\n    return mixed_x1, mixed_x2, y_a, y_b, index, lam\n\ndef evaluate_with_demographics(model, dataloader, device, threshold=0.5):\n    model.eval()\n    all_probs, all_labels, all_genders, all_races = [], [], [], []\n    with torch.no_grad():\n        for oct_img, slo_img, lbls in dataloader:\n            oct_img = oct_img.to(device); slo_img = slo_img.to(device); lbls = lbls.to(device)\n            glaucoma_label_true = lbls[:,0].view(-1).cpu().numpy()\n            genders = lbls[:,1].view(-1).cpu().numpy()\n            races   = lbls[:,4].view(-1).cpu().numpy() # Race is at index 4\n\n            out = model.negative_elbo_bound(oct_img, slo_img, lbls, sample=True)\n            # out indices: 0 nelbo,1 kl,2 rec,3 decoded,4 z_given,5 glaucoma_loss,6 fused,7 scoring,8 fused_scored,9 e_flat\n            fused_scored = out[8]\n            e_flat = out[9]\n\n            classifier_input = torch.cat([fused_scored, e_flat], dim=1)\n            try:\n                first_clf_param = next(model.glaucoma_predictor.parameters())\n                clf_dtype = first_clf_param.dtype\n            except StopIteration:\n                clf_dtype = classifier_input.dtype\n            if classifier_input.dtype != clf_dtype:\n                classifier_input = classifier_input.to(clf_dtype)\n\n            logits = model.glaucoma_predictor(classifier_input)\n            probs = torch.sigmoid(logits).view(-1).cpu().numpy()\n            all_probs.append(probs); all_labels.append(glaucoma_label_true)\n            all_genders.append(genders); all_races.append(races)\n\n    if len(all_probs) == 0:\n        return {}, np.array([]), np.array([]), np.array([])\n    all_probs = np.concatenate(all_probs)\n    all_labels = np.concatenate(all_labels).astype(int)\n    all_genders = np.concatenate(all_genders).astype(int)\n    all_races = np.concatenate(all_races).astype(int)\n\n    def compute_metrics(y_true, y_probs):\n        if len(y_true) == 0:\n            return {'accuracy':np.nan, 'f1_score':np.nan, 'roc_auc':np.nan, 'sensitivity':np.nan, 'specificity':np.nan}\n        preds = (y_probs > threshold).astype(int)\n        m = {}\n        m['accuracy'] = float(accuracy_score(y_true, preds))\n        m['f1_score'] = float(f1_score(y_true, preds, zero_division=0))\n        # Sensitivity (Recall)\n        m['sensitivity'] = float(recall_score(y_true, preds, zero_division=0))\n        # Specificity\n        tn, fp, fn, tp = confusion_matrix(y_true, preds, labels=[0,1]).ravel()\n        m['specificity'] = float(tn / (tn + fp)) if (tn + fp) > 0 else 0.0\n        # AUC\n        if len(np.unique(y_true)) > 1:\n            m['roc_auc'] = float(roc_auc_score(y_true, y_probs))\n        else:\n            m['roc_auc'] = np.nan\n        return m\n\n    # Calculate metrics for groups\n    results = {}\n    results['Overall'] = compute_metrics(all_labels, all_probs)\n\n    # Gender Groups\n    results['Male'] = compute_metrics(all_labels[all_genders == 0], all_probs[all_genders == 0])\n    results['Female'] = compute_metrics(all_labels[all_genders == 1], all_probs[all_genders == 1])\n\n    # Race Groups (0=White, 1=Black, 2=Asian)\n    results['White'] = compute_metrics(all_labels[all_races == 0], all_probs[all_races == 0])\n    results['Black'] = compute_metrics(all_labels[all_races == 1], all_probs[all_races == 1])\n    results['Asian'] = compute_metrics(all_labels[all_races == 2], all_probs[all_races == 2])\n\n    return results, all_labels, all_probs, all_genders\n\n# -------------------------\n# Training loop without validation\n# -------------------------\ndef train_full():\n    # ensure dataset present\n    if not (os.path.exists(TRAIN_DIR) and os.path.exists(TEST_DIR)):\n        download_and_extract_fairdomain()\n    else:\n        print(\"Training/testing directories already present; skipping download.\")\n    clone_trustworthyai()\n\n    try:\n        import utils as ut\n        print(\"Imported trustworthyAI utils.\")\n    except Exception:\n        ut = None\n        print(\"Could not import trustworthyAI utils; continuing with fallbacks.\")\n\n    # Load summary CSV for demographics\n    summary_map = load_summary_csv_as_map(SUMMARY_CSV_PATH)\n\n    print(\"Loading .npz from:\", TRAIN_DIR, TEST_DIR)\n    training_data = load_npz_dir(TRAIN_DIR)\n    testing_data = load_npz_dir(TEST_DIR)\n    print(\"Found .npz - train:\", len(training_data), \" test:\", len(testing_data))\n\n    # Process data with summary map to get race/gender correctly\n    oct_train_arr, slo_train_arr, labels_train_arr = process_dataset_data_to_arrays(training_data, summary_map)\n    oct_test_arr, slo_test_arr, labels_test_arr = process_dataset_data_to_arrays(testing_data, summary_map)\n    print(\"Processed arrays - train:\", len(oct_train_arr), \" test:\", len(oct_test_arr))\n\n    if len(oct_train_arr) == 0:\n        raise RuntimeError(\"No training data; check extraction.\")\n\n    # input size: efficientnet_b3 expects 300, else use 224\n    input_size = 300 if \"efficientnet_b3\" in BACKBONE else 224\n\n    try:\n        randaug = transforms.RandAugment(num_ops=2, magnitude=9)\n    except Exception:\n        randaug = None\n\n    train_transform = transforms.Compose([\n        transforms.Resize((input_size,input_size)),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomRotation(10),\n        transforms.RandomResizedCrop(input_size, scale=(0.85,1.0)),\n        randaug if randaug is not None else transforms.Identity(),\n        transforms.ColorJitter(brightness=0.12, contrast=0.12, saturation=0.06),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[INPUT_MEAN], std=[INPUT_STD])\n    ])\n    test_transform = transforms.Compose([\n        transforms.Resize((input_size,input_size)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[INPUT_MEAN], std=[INPUT_STD])\n    ])\n\n    # Use full provided training set (no validation split)\n    train_ds = DualImageDataset(oct_train_arr, slo_train_arr, labels_train_arr, transform=train_transform)\n    test_ds = DualImageDataset(oct_test_arr, slo_test_arr, labels_test_arr, transform=test_transform)\n\n    glaucoma_labels = labels_train_arr[:,0]\n    class_counts = np.array([np.sum(glaucoma_labels==0), np.sum(glaucoma_labels==1)])\n    print(\"Glaucoma counts (train):\", class_counts)\n    inv_freq = 1.0 / (class_counts + 1e-8)\n    sample_weights = np.array([inv_freq[int(l)] for l in glaucoma_labels])\n    sampler = WeightedRandomSampler(torch.from_numpy(sample_weights).double(), num_samples=len(sample_weights), replacement=True)\n\n    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=sampler, num_workers=2, pin_memory=True)\n    test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n\n    model = CausalVAE_AttnScoring_Improved(ut_module=ut, z1_dim=Z1_DIM, z2_dim=Z2_DIM, proj_dim=PROJ_DIM, device=DEVICE, backbone=BACKBONE)\n    model.to(DEVICE)\n\n    def compute_pos_weight(arr):\n        a = arr if isinstance(arr, np.ndarray) else arr.cpu().numpy()\n        glaucoma = a[:,0] if a.size>0 else np.array([])\n        if glaucoma.size == 0: return 1.0\n        pos = (glaucoma==1).sum(); neg = (glaucoma==0).sum()\n        if pos == 0: return 1.0\n        return float(neg) / (float(pos) + 1e-8)\n    model.pos_weight = torch.tensor(compute_pos_weight(labels_train_arr)).float().to(DEVICE)\n    print(\"model.pos_weight:\", float(model.pos_weight))\n\n    # Stage1: freeze most params; unfreeze projection/gate/fusion_out, adj mapping, classifier\n    for p in model.parameters(): p.requires_grad = False\n    try:\n        for p in model.enc.proj_o.parameters(): p.requires_grad = True\n        for p in model.enc.proj_s.parameters(): p.requires_grad = True\n        for p in model.enc.gate.parameters(): p.requires_grad = True\n        for p in model.enc.fusion_out.parameters(): p.requires_grad = True\n    except Exception:\n        pass\n    for p in model.adj_to_feat.parameters(): p.requires_grad = True\n    for p in model.glaucoma_predictor.parameters(): p.requires_grad = True\n\n    params_stage1 = [p for p in model.parameters() if p.requires_grad]\n    opt1 = optim.AdamW(params_stage1, lr=HEAD_LR, weight_decay=WEIGHT_DECAY)\n    sched1 = torch.optim.lr_scheduler.CosineAnnealingLR(opt1, T_max=max(1,NUM_EPOCHS_STAGE1), eta_min=1e-6)\n    scaler = torch.amp.GradScaler()\n\n    best_train_loss = float('inf')\n    best_wts = copy.deepcopy(model.state_dict())\n    history = {\"train_loss\": []}\n\n    def train_one_epoch(loader, optimizer, epoch_idx):\n        model.train()\n        running_loss = 0.0; n_batches = 0\n        for oct_img, slo_img, lbls in loader:\n            oct_img = oct_img.to(DEVICE); slo_img = slo_img.to(DEVICE); lbls = lbls.to(DEVICE)\n            optimizer.zero_grad()\n            if USE_MIXUP:\n                mx1, mx2, ya, yb, index, lam = mixup_data(oct_img, slo_img, lbls, alpha=MIXUP_ALPHA, device=DEVICE)\n                with torch.amp.autocast(device_type=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n                    # negative_elbo_bound now returns 10 items\n                    nelbo, kl, rec, dec, z_given, glaucoma_loss, fused, scoring, fused_scored, e_flat = model.negative_elbo_bound(mx1, mx2, ya, kl_weight = min(1.0, epoch_idx/max(1,KL_ANNEAL_EPOCHS)))\n                # build classifier input for mixup case same as in model\n                classifier_input = torch.cat([fused_scored, e_flat], dim=1)\n                try:\n                    first_clf_param = next(model.glaucoma_predictor.parameters())\n                    clf_dtype = first_clf_param.dtype\n                except StopIteration:\n                    clf_dtype = classifier_input.dtype\n                if classifier_input.dtype != clf_dtype:\n                    classifier_input = classifier_input.to(clf_dtype)\n                logits = model.glaucoma_predictor(classifier_input)\n                target_a = ya[:,0].view(-1,1)\n                target_b = yb[:,0].view(-1,1)\n                loss_a = F.binary_cross_entropy_with_logits(logits, target_a, reduction='mean')\n                loss_b = F.binary_cross_entropy_with_logits(logits, target_b, reduction='mean')\n                glaucoma_loss_mixed = lam * loss_a + (1-lam) * loss_b\n                loss = rec + kl + glaucoma_loss_mixed\n            else:\n                with torch.amp.autocast(device_type=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n                    loss, sums = model.loss(oct_img, slo_img, lbls, kl_weight = min(1.0, epoch_idx/max(1,KL_ANNEAL_EPOCHS)))\n            if not torch.isfinite(loss): continue\n            scaler.scale(loss).backward()\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n            scaler.step(optimizer); scaler.update()\n            loss_value = loss.detach().cpu().item()\n            running_loss += loss_value; n_batches += 1\n        return running_loss / (n_batches + 1e-8)\n\n    print(\"Starting Stage 1 warmup for %d epochs (HEAD_LR=%g)\" % (NUM_EPOCHS_STAGE1, HEAD_LR))\n    for e in range(1, NUM_EPOCHS_STAGE1+1):\n        tloss = train_one_epoch(train_loader, opt1, e)\n        try: sched1.step()\n        except Exception: pass\n        history['train_loss'].append(tloss)\n        print(f\"Stage1 Epoch {e}/{NUM_EPOCHS_STAGE1} - train_loss {tloss:.4f}\")\n        if tloss < best_train_loss:\n            best_train_loss = tloss; best_wts = copy.deepcopy(model.state_dict())\n            torch.save({'model_state_dict':best_wts}, \"best_model_stage1.pt\")\n            print(\"Saved best (stage1)\")\n\n    for p in model.parameters(): p.requires_grad = True\n\n    head_params = list(model.adj_to_feat.parameters()) + list(model.glaucoma_predictor.parameters())\n    try:\n        head_params += list(model.enc.fusion_out.parameters())\n    except Exception:\n        pass\n    head_param_set = set(head_params)\n    head_group = [p for p in model.parameters() if p in head_param_set]\n    other_group = [p for p in model.parameters() if p not in head_param_set]\n\n    opt2 = optim.AdamW([{'params': head_group, 'lr': HEAD_LR}, {'params': other_group, 'lr': BACKBONE_LR}], weight_decay=WEIGHT_DECAY)\n\n    try:\n        steps_per_epoch = max(1, len(train_loader))\n        total_steps = max(1, NUM_EPOCHS_STAGE2 * steps_per_epoch)\n        sched2 = torch.optim.lr_scheduler.OneCycleLR(opt2, max_lr=HEAD_LR, total_steps=total_steps)\n        use_batch_sched = True\n    except Exception:\n        sched2 = torch.optim.lr_scheduler.CosineAnnealingLR(opt2, T_max=max(1,NUM_EPOCHS_STAGE2), eta_min=1e-6)\n        use_batch_sched = False\n\n    swa_model = None; swa_scheduler = None\n    if USE_SWA:\n        try:\n            from torch.optim.swa_utils import AveragedModel, SWALR\n            swa_model = AveragedModel(model)\n            swa_scheduler = SWALR(opt2, swa_lr=1e-4)\n            print(\"SWA available; will run SWA starting epoch\", SWA_START_EPOCH)\n        except Exception:\n            swa_model = None; swa_scheduler = None\n\n    print(\"Starting Stage 2 joint training for %d epochs (BACKBONE_LR=%g)\" % (NUM_EPOCHS_STAGE2, BACKBONE_LR))\n    no_improve = 0\n    for e in range(1, NUM_EPOCHS_STAGE2+1):\n        tloss = train_one_epoch(train_loader, opt2, e)\n        if use_batch_sched:\n            try: sched2.step()\n            except Exception: pass\n        else:\n            try: sched2.step()\n            except Exception: pass\n\n        if swa_model is not None and e >= SWA_START_EPOCH:\n            swa_model.update_parameters(model)\n            if swa_scheduler is not None:\n                try: swa_scheduler.step()\n                except Exception: pass\n\n        history['train_loss'].append(tloss)\n        print(f\"Stage2 Epoch {e}/{NUM_EPOCHS_STAGE2} - train_loss {tloss:.4f}\")\n        if tloss < best_train_loss - 1e-6:\n            best_train_loss = tloss; best_wts = copy.deepcopy(model.state_dict()); torch.save({'model_state_dict':best_wts}, \"best_model_stage2.pt\")\n            print(\"Saved best (stage2)\"); no_improve = 0\n        else:\n            no_improve += 1\n            if no_improve >= EARLY_STOP_PATIENCE:\n                print(\"Early stopping (no improvement).\"); break\n\n    if swa_model is not None:\n        try:\n            from torch.optim.swa_utils import update_bn\n            print(\"Applying SWA averaged weights and updating BN stats...\")\n            model.load_state_dict(swa_model.module.state_dict() if hasattr(swa_model, \"module\") else swa_model.state_dict())\n            update_bn(train_loader, model, device=DEVICE)\n        except Exception as e:\n            print(\"SWA finalization failed:\", e)\n\n    model.load_state_dict(best_wts)\n    print(\"Training complete. Best train loss:\", best_train_loss)\n\n    # Evaluate on test set using default threshold 0.5\n    test_metrics_dict, labels_test_arr, probs_test_arr, genders_test_arr = evaluate_with_demographics(model, test_loader, DEVICE, threshold=0.5)\n\n    print(\"\\n--- Final Test Metrics ---\")\n    for group, metrics in test_metrics_dict.items():\n        print(f\"\\nGroup: {group}\")\n        for k, v in metrics.items():\n            print(f\"  {k}: {v:.4f}\")\n\n    return model, test_metrics_dict, history\n\nif __name__ == \"__main__\":\n    model, test_metrics, history = train_full()\n    try:\n        plt.figure(figsize=(8,4))\n        plt.plot(history['train_loss']); plt.title('Train loss (per logged entry)'); plt.grid(True)\n        plt.xlabel('Iteration'); plt.ylabel('Loss')\n        plt.tight_layout(); plt.show()\n    except Exception as e:\n        print(\"Plot failed:\", e)\n        traceback.print_exc()\n    print(\"Final test metrics dict:\")\n    print(test_metrics)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"c8IGSqn6VTd6","outputId":"e5765ab3-e99b-4964-f0bb-f2177f5bbce2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training/testing directories already present; skipping download.\n","trustworthyAI already present at /content/trustworthyAI\n","Imported trustworthyAI utils.\n","Loading .npz from: dataset_extracted/Training dataset_extracted/Testing\n","Found .npz - train: 8000  test: 2000\n","Processed arrays - train: 8000  test: 2000\n","Glaucoma counts (train): [4434 3566]\n","Downloading: \"https://download.pytorch.org/models/efficientnet_b3_rwightman-b3899882.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b3_rwightman-b3899882.pth\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 47.2M/47.2M [00:00<00:00, 150MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["DAG utilities loaded.\n","model.pos_weight: 1.2434099912643433\n","Starting Stage 1 warmup for 10 epochs (HEAD_LR=0.001)\n","Stage1 Epoch 1/10 - train_loss 1.3502\n","Saved best (stage1)\n","Stage1 Epoch 2/10 - train_loss 1.3165\n","Saved best (stage1)\n","Stage1 Epoch 3/10 - train_loss 1.3197\n","Stage1 Epoch 4/10 - train_loss 1.3086\n","Saved best (stage1)\n","Stage1 Epoch 5/10 - train_loss 1.3055\n","Saved best (stage1)\n","Stage1 Epoch 6/10 - train_loss 1.3019\n","Saved best (stage1)\n","Stage1 Epoch 7/10 - train_loss 1.3019\n","Saved best (stage1)\n","Stage1 Epoch 8/10 - train_loss 1.2962\n","Saved best (stage1)\n","Stage1 Epoch 9/10 - train_loss 1.2942\n","Saved best (stage1)\n","Stage1 Epoch 10/10 - train_loss 1.2984\n","Starting Stage 2 joint training for 50 epochs (BACKBONE_LR=3e-05)\n","Stage2 Epoch 1/50 - train_loss 1.2716\n","Saved best (stage2)\n","Stage2 Epoch 2/50 - train_loss 1.2391\n","Saved best (stage2)\n","Stage2 Epoch 3/50 - train_loss 1.2268\n","Saved best (stage2)\n","Stage2 Epoch 4/50 - train_loss 1.2087\n","Saved best (stage2)\n","Stage2 Epoch 5/50 - train_loss 1.2118\n","Stage2 Epoch 6/50 - train_loss 1.1795\n","Saved best (stage2)\n","Stage2 Epoch 7/50 - train_loss 1.1609\n","Saved best (stage2)\n","Stage2 Epoch 8/50 - train_loss 1.1553\n","Saved best (stage2)\n","Stage2 Epoch 9/50 - train_loss 1.1498\n","Saved best (stage2)\n","Stage2 Epoch 10/50 - train_loss 1.1352\n","Saved best (stage2)\n","Stage2 Epoch 11/50 - train_loss 1.1327\n","Saved best (stage2)\n","Stage2 Epoch 12/50 - train_loss 1.1200\n","Saved best (stage2)\n","Stage2 Epoch 13/50 - train_loss 1.0995\n","Saved best (stage2)\n","Stage2 Epoch 14/50 - train_loss 1.0878\n","Saved best (stage2)\n","Stage2 Epoch 15/50 - train_loss 1.0956\n","Stage2 Epoch 16/50 - train_loss 1.0801\n","Saved best (stage2)\n","Stage2 Epoch 17/50 - train_loss 1.0710\n","Saved best (stage2)\n","Stage2 Epoch 18/50 - train_loss 1.0517\n","Saved best (stage2)\n","Stage2 Epoch 19/50 - train_loss 1.0449\n","Saved best (stage2)\n","Stage2 Epoch 20/50 - train_loss 1.0303\n","Saved best (stage2)\n","Stage2 Epoch 21/50 - train_loss 1.0263\n","Saved best (stage2)\n","Stage2 Epoch 22/50 - train_loss 1.0172\n","Saved best (stage2)\n","Stage2 Epoch 23/50 - train_loss 1.0220\n","Stage2 Epoch 24/50 - train_loss 0.9889\n","Saved best (stage2)\n","Stage2 Epoch 25/50 - train_loss 0.9839\n","Saved best (stage2)\n","Stage2 Epoch 26/50 - train_loss 1.0012\n","Stage2 Epoch 27/50 - train_loss 1.0048\n","Stage2 Epoch 28/50 - train_loss 0.9771\n","Saved best (stage2)\n","Stage2 Epoch 29/50 - train_loss 0.9538\n","Saved best (stage2)\n","Stage2 Epoch 30/50 - train_loss 0.9735\n","Stage2 Epoch 31/50 - train_loss 0.9643\n","Stage2 Epoch 32/50 - train_loss 0.9705\n","Stage2 Epoch 33/50 - train_loss 0.9573\n","Stage2 Epoch 34/50 - train_loss 0.9499\n","Saved best (stage2)\n","Stage2 Epoch 35/50 - train_loss 0.9440\n","Saved best (stage2)\n","Stage2 Epoch 36/50 - train_loss 0.9145\n","Saved best (stage2)\n","Stage2 Epoch 37/50 - train_loss 0.9638\n","Stage2 Epoch 38/50 - train_loss 0.9222\n","Stage2 Epoch 39/50 - train_loss 0.9373\n","Stage2 Epoch 40/50 - train_loss 0.9625\n","Stage2 Epoch 41/50 - train_loss 0.9378\n","Stage2 Epoch 42/50 - train_loss 0.9307\n","Stage2 Epoch 43/50 - train_loss 0.9072\n","Saved best (stage2)\n","Stage2 Epoch 44/50 - train_loss 0.9535\n","Stage2 Epoch 45/50 - train_loss 0.8967\n","Saved best (stage2)\n","Stage2 Epoch 46/50 - train_loss 0.9367\n","Stage2 Epoch 47/50 - train_loss 0.8955\n","Saved best (stage2)\n","Stage2 Epoch 48/50 - train_loss 0.8947\n","Saved best (stage2)\n","Stage2 Epoch 49/50 - train_loss 0.9193\n","Stage2 Epoch 50/50 - train_loss 0.9078\n","Training complete. Best train loss: 0.8946720570236443\n","\n","--- Final Test Metrics ---\n","\n","Group: Overall\n","  accuracy: 0.7625\n","  f1_score: 0.7442\n","  sensitivity: 0.7790\n","  specificity: 0.7493\n","  roc_auc: 0.8529\n","\n","Group: Male\n","  accuracy: 0.7586\n","  f1_score: 0.7324\n","  sensitivity: 0.7583\n","  specificity: 0.7588\n","  roc_auc: 0.8533\n","\n","Group: Female\n","  accuracy: 0.7640\n","  f1_score: 0.7485\n","  sensitivity: 0.7867\n","  specificity: 0.7456\n","  roc_auc: 0.8527\n","\n","Group: White\n","  accuracy: 0.7608\n","  f1_score: 0.7234\n","  sensitivity: 0.7765\n","  specificity: 0.7503\n","  roc_auc: 0.8514\n","\n","Group: Black\n","  accuracy: 0.7542\n","  f1_score: 0.8021\n","  sensitivity: 0.7812\n","  specificity: 0.7064\n","  roc_auc: 0.8327\n","\n","Group: Asian\n","  accuracy: 0.7910\n","  f1_score: 0.7784\n","  sensitivity: 0.7927\n","  specificity: 0.7895\n","  roc_auc: 0.8846\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 800x400 with 1 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Final test metrics dict:\n","{'Overall': {'accuracy': 0.7625, 'f1_score': 0.7442110931610124, 'sensitivity': 0.7790304396843292, 'specificity': 0.7493261455525606, 'roc_auc': 0.8529320898553632}, 'Male': {'accuracy': 0.7586206896551724, 'f1_score': 0.7323943661971831, 'sensitivity': 0.7583333333333333, 'specificity': 0.7588424437299035, 'roc_auc': 0.8532556270096463}, 'Female': {'accuracy': 0.7639751552795031, 'f1_score': 0.7485294117647059, 'sensitivity': 0.7867078825347759, 'specificity': 0.7456359102244389, 'roc_auc': 0.8527001661225606}, 'White': {'accuracy': 0.7608409986859396, 'f1_score': 0.723404255319149, 'sensitivity': 0.7765089722675367, 'specificity': 0.7502750275027503, 'roc_auc': 0.8514384880576149}, 'Black': {'accuracy': 0.7541528239202658, 'f1_score': 0.8021390374331551, 'sensitivity': 0.78125, 'specificity': 0.7064220183486238, 'roc_auc': 0.832664373088685}, 'Asian': {'accuracy': 0.7909604519774012, 'f1_score': 0.7784431137724551, 'sensitivity': 0.7926829268292683, 'specificity': 0.7894736842105263, 'roc_auc': 0.8845956354300385}}\n"]}],"execution_count":null},{"cell_type":"markdown","source":"# Baseline single ResNet34 encoder + MLP","metadata":{"id":"Vl0xYAJGRq7n"}},{"cell_type":"code","source":"# combined_baseline_resnet34_fix.py\n# Baseline: Single ResNet34 (Shared Weights) + MLP\n# Fix: Added custom_update_bn to handle SWA with two input images\n\nimport os, sys, zipfile, glob, subprocess, copy, math, time, traceback\nfrom typing import Tuple, Dict, Any\n\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport torchvision.models as models\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n\nimport matplotlib.pyplot as plt\n\n# -------------------------\n# Config\n# -------------------------\nGDRIVE_LINK = \"Drive link to the fairdomain zip file\"\nFAIRDOMAIN_ZIP = \"FairDomain.zip\"\nEXTRACT_ROOT = \"dataset_extracted\"\nTRAIN_DIR = os.path.join(EXTRACT_ROOT, \"Training\")\nTEST_DIR  = os.path.join(EXTRACT_ROOT, \"Testing\")\nSUMMARY_CSV_PATH = \"/content/data_summary.csv\"\n\n# Model/training hyperparameters\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nSEED = 42\nBATCH_SIZE = 24\nNUM_EPOCHS_STAGE1 = 10  # Head only\nNUM_EPOCHS_STAGE2 = 50  # Fine-tuning\nHEAD_LR = 1e-3\nBACKBONE_LR = 3e-5\nWEIGHT_DECAY = 1e-5\nINPUT_MEAN = 0.5\nINPUT_STD  = 0.25\n\nGLAUCOMA_MLP_HIDDEN = 256\nMAX_GRAD_NORM = 1.0\nEARLY_STOP_PATIENCE = 12\n\nUSE_MIXUP = True\nMIXUP_ALPHA = 0.2\nUSE_SWA = True\nSWA_START_EPOCH = max(1, NUM_EPOCHS_STAGE2 - 5)\n# -------------------------\n\ndef run_shell(cmd):\n    print(\"RUN:\", cmd)\n    r = subprocess.run(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    if r.returncode != 0:\n        print(r.stderr.decode(\"utf-8\"))\n    return r\n\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\n# -------------------------\n# Data Setup Helpers\n# -------------------------\ndef download_and_extract_fairdomain(gdrive_link=GDRIVE_LINK, out_zip=FAIRDOMAIN_ZIP):\n    try:\n        import gdown\n    except Exception:\n        print(\"Installing gdown...\")\n        run_shell(\"pip install -q gdown\")\n        import gdown\n    parts = gdrive_link.split(\"/\")\n    file_id = None\n    for i,p in enumerate(parts):\n        if p == \"d\" and i+1 < len(parts):\n            file_id = parts[i+1]; break\n    if file_id is None:\n        file_id = gdrive_link.split(\"/\")[-2]\n    print(\"Downloading FairDomain.zip (id=%s)...\" % file_id)\n    gdown.download(id=file_id, output=out_zip, quiet=False)\n    if not os.path.exists(out_zip):\n        raise RuntimeError(\"Download failed: %s\" % out_zip)\n    print(\"Extracting FairDomain.zip ...\")\n    with zipfile.ZipFile(out_zip, \"r\") as z:\n        z.extractall(\".\")\n    candidates = glob.glob(\"**/dataset*.zip\", recursive=True)\n    if not candidates:\n        candidates = glob.glob(\"**/dataset.zip\", recursive=True)\n    if not candidates:\n        raise RuntimeError(\"Could not find dataset.zip inside FairDomain.zip extracted contents.\")\n    dataset_zip = candidates[0]\n    print(\"Found dataset:\", dataset_zip)\n    os.makedirs(EXTRACT_ROOT, exist_ok=True)\n    with zipfile.ZipFile(dataset_zip, \"r\") as z:\n        z.extractall(EXTRACT_ROOT)\n    print(\"Dataset extracted to\", EXTRACT_ROOT)\n\ndef normalize_race_value(rv) -> int:\n    \"\"\" Maps race inputs to integers: 0=White, 1=Black, 2=Asian, 3=Other \"\"\"\n    if rv is None: return 3\n    try:\n        if isinstance(rv, (str, bytes, bytearray)):\n             s = str(rv).strip().lower()\n             if 'white' in s: return 0\n             if 'black' in s or 'african' in s: return 1\n             if 'asian' in s: return 2\n             return 3\n        if np.isscalar(rv):\n            if int(rv) in (0,1,2,3): return int(rv)\n            return 3\n        return 3\n    except Exception:\n        return 3\n\ndef load_summary_csv_as_map(csv_path: str) -> Dict[str, Dict[str, Any]]:\n    if not os.path.exists(csv_path):\n        print(f\"Warning: Summary CSV not found at {csv_path}\")\n        return {}\n    try:\n        df = pd.read_csv(csv_path)\n    except Exception as e:\n        print(f\"Error reading CSV: {e}\")\n        return {}\n\n    possible_id_cols = [c for c in df.columns if c.lower() in ('id','subject_id','patient_id','filename','file','case','name')]\n    chosen_id_col = possible_id_cols[0] if possible_id_cols else None\n\n    if chosen_id_col is None: return {}\n\n    df['_key'] = df[chosen_id_col].astype(str).str.strip()\n    mapping = {}\n\n    for _, row in df.iterrows():\n        key = row['_key']\n        key_clean = os.path.splitext(key)[0]\n        mapping[key] = row.to_dict()\n        mapping[key_clean] = row.to_dict()\n\n    return mapping\n\ndef load_npz_dir(root_dir):\n    out = {}\n    if not os.path.exists(root_dir):\n        print(\"Warning: path does not exist:\", root_dir)\n        return out\n    for root, _, files in os.walk(root_dir):\n        for f in files:\n            if f.endswith(\".npz\"):\n                p = os.path.join(root, f)\n                k = os.path.splitext(f)[0]\n                try:\n                    data = np.load(p, allow_pickle=True)\n                    out[k] = dict(data)\n                except Exception as e:\n                    print(\"Failed to load\", p, e)\n    return out\n\ndef process_dataset_data_to_arrays(data_dict, summary_map: Dict[str, Dict[str, Any]] = None):\n    octs, slos, labels = [], [], []\n    keys = sorted(data_dict.keys())\n    for k in keys:\n        d = data_dict[k]\n        if \"oct_fundus\" not in d or \"slo_fundus\" not in d:\n            continue\n        o = d[\"oct_fundus\"]; s = d[\"slo_fundus\"]\n        if o.ndim == 3 and o.shape[-1] == 1: o = np.squeeze(o, axis=-1)\n        if s.ndim == 3 and s.shape[-1] == 1: s = np.squeeze(s, axis=-1)\n\n        glaucoma = float(d.get(\"glaucoma\", 0.0))\n        gender = float(d.get(\"gender\", 0.0))\n        age = float(d.get(\"age\", 0.0))\n        md = float(d.get(\"md\", 0.0))\n        race_val = d.get(\"race\", None)\n\n        if summary_map and k in summary_map:\n            entry = summary_map[k]\n            if 'glaucoma' in entry and pd.notna(entry['glaucoma']):\n                g_val = str(entry['glaucoma']).lower()\n                glaucoma = 1.0 if g_val in ['yes', '1', 'true'] else 0.0\n            if 'gender' in entry and pd.notna(entry['gender']):\n                g_val = str(entry['gender']).lower()\n                gender = 1.0 if g_val in ['female', 'f', '1'] else 0.0\n            if 'race' in entry and pd.notna(entry['race']):\n                race_val = entry['race']\n            if 'age' in entry and pd.notna(entry['age']):\n                try: age = float(entry['age'])\n                except: pass\n            if 'md' in entry and pd.notna(entry['md']):\n                try: md = float(entry['md'])\n                except: pass\n\n        race_code = normalize_race_value(race_val)\n\n        octs.append(o.astype(np.float32)); slos.append(s.astype(np.float32))\n        labels.append([glaucoma, gender, age, md, float(race_code)])\n\n    if len(octs) == 0: return np.array([]), np.array([]), np.array([])\n    return np.array(octs), np.array(slos), np.array(labels, dtype=np.float32)\n\nclass DualImageDataset(Dataset):\n    def __init__(self, oct_images, slo_images, labels, transform=None):\n        assert len(oct_images) == len(slo_images) == len(labels)\n        self.oct_images = oct_images; self.slo_images = slo_images; self.labels = labels; self.transform = transform\n    def _to_pil(self,a):\n        arr = a\n        if arr.dtype != np.uint8:\n            if arr.max() <= 1.0: arr = (arr*255.0).astype(np.uint8)\n            else: arr = arr.astype(np.uint8)\n        return Image.fromarray(arr)\n    def __len__(self): return len(self.oct_images)\n    def __getitem__(self, idx):\n        o = self.oct_images[idx]; s = self.slo_images[idx]\n        o_pil = self._to_pil(o); s_pil = self._to_pil(s)\n        if self.transform:\n            o_t = self.transform(o_pil); s_t = self.transform(s_pil)\n        else:\n            o_t = transforms.ToTensor()(o_pil); s_t = transforms.ToTensor()(s_pil)\n        lbl = torch.tensor(self.labels[idx], dtype=torch.float32)\n        return o_t, s_t, lbl\n\n# -------------------------\n# Custom Update BN for SWA\n# -------------------------\ndef custom_update_bn(loader, model, device=None):\n    \"\"\"\n    Custom update_bn that handles dataloader returning (oct, slo, label).\n    Standard torch.optim.swa_utils.update_bn expects (input, label)\n    and passes input directly to model(input).\n    \"\"\"\n    if device is None:\n        device = next(model.parameters()).device\n\n    momenta = {}\n    for module in model.modules():\n        if isinstance(module, torch.nn.modules.batchnorm._BatchNorm):\n            module.running_mean = torch.zeros_like(module.running_mean)\n            module.running_var = torch.ones_like(module.running_var)\n            momenta[module] = module.momentum\n            module.momentum = None\n            module.num_batches_tracked *= 0\n\n    model.train()\n    n = 0\n    with torch.no_grad():\n        for oct_img, slo_img, _ in loader:\n            oct_img = oct_img.to(device)\n            slo_img = slo_img.to(device)\n            # Forward pass to update BN stats\n            model(oct_img, slo_img)\n            n += 1\n\n    for module in model.modules():\n        if isinstance(module, torch.nn.modules.batchnorm._BatchNorm):\n            module.momentum = momenta[module]\n\n# -------------------------\n# New Baseline Architecture\n# -------------------------\nclass GlaucomaPredictorMLP(nn.Module):\n    def __init__(self, input_dim, hidden_dim=GLAUCOMA_MLP_HIDDEN, dropout=0.4):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.BatchNorm1d(hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, max(8, hidden_dim//2)),\n            nn.BatchNorm1d(max(8, hidden_dim//2)),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout/2),\n            nn.Linear(max(8, hidden_dim//2), 1)\n        )\n    def forward(self,x): return self.net(x)\n\nclass BaselineResNet34(nn.Module):\n    def __init__(self, device=DEVICE):\n        super().__init__()\n        # Load ResNet34\n        try:\n            from torchvision.models import ResNet34_Weights\n            weights = ResNet34_Weights.IMAGENET1K_V1\n            self.backbone = models.resnet34(weights=weights)\n        except:\n            self.backbone = models.resnet34(pretrained=True)\n\n        # Modify first layer for 1 channel (grayscale)\n        old_conv = self.backbone.conv1\n        new_conv = nn.Conv2d(1, old_conv.out_channels,\n                           kernel_size=old_conv.kernel_size,\n                           stride=old_conv.stride,\n                           padding=old_conv.padding,\n                           bias=old_conv.bias)\n        # Average weights to keep pretrained info\n        with torch.no_grad():\n            new_conv.weight[:] = old_conv.weight.mean(dim=1, keepdim=True)\n        self.backbone.conv1 = new_conv\n\n        # Replace FC with Identity to get features\n        self.feat_dim = self.backbone.fc.in_features # 512 for ResNet34\n        self.backbone.fc = nn.Identity()\n\n        # MLP Classifier taking concatenated features (512 * 2 = 1024)\n        self.classifier = GlaucomaPredictorMLP(input_dim=self.feat_dim*2, hidden_dim=GLAUCOMA_MLP_HIDDEN)\n\n        self.pos_weight = None\n\n    def extract_features(self, x):\n        if x.dim() == 3: x = x.unsqueeze(1)\n        return self.backbone(x)\n\n    def forward(self, oct_img, slo_img):\n        # Shared encoder used twice\n        f_oct = self.extract_features(oct_img)\n        f_slo = self.extract_features(slo_img)\n\n        # Concatenate features\n        combined = torch.cat([f_oct, f_slo], dim=1)\n\n        # Classify\n        logits = self.classifier(combined)\n        return logits\n\ndef mixup_data(x1, x2, y, alpha=0.2, device=DEVICE):\n    if alpha <= 0:\n        return x1, x2, y, None, None, 1.0\n    lam = np.random.beta(alpha, alpha)\n    batch_size = x1.size(0)\n    index = torch.randperm(batch_size).to(device)\n    mixed_x1 = lam * x1 + (1 - lam) * x1[index, :]\n    mixed_x2 = lam * x2 + (1 - lam) * x2[index, :]\n    y_a, y_b = y, y[index]\n    return mixed_x1, mixed_x2, y_a, y_b, index, lam\n\ndef evaluate_with_demographics(model, dataloader, device, threshold=0.5):\n    model.eval()\n    all_probs, all_labels, all_genders, all_races = [], [], [], []\n    with torch.no_grad():\n        for oct_img, slo_img, lbls in dataloader:\n            oct_img = oct_img.to(device); slo_img = slo_img.to(device); lbls = lbls.to(device)\n            glaucoma_label_true = lbls[:,0].view(-1).cpu().numpy()\n            genders = lbls[:,1].view(-1).cpu().numpy()\n            races   = lbls[:,4].view(-1).cpu().numpy()\n\n            logits = model(oct_img, slo_img)\n            probs = torch.sigmoid(logits).view(-1).cpu().numpy()\n\n            all_probs.append(probs); all_labels.append(glaucoma_label_true)\n            all_genders.append(genders); all_races.append(races)\n\n    if len(all_probs) == 0:\n        return {}, np.array([]), np.array([]), np.array([])\n    all_probs = np.concatenate(all_probs)\n    all_labels = np.concatenate(all_labels).astype(int)\n    all_genders = np.concatenate(all_genders).astype(int)\n    all_races = np.concatenate(all_races).astype(int)\n\n    def compute_metrics(y_true, y_probs):\n        if len(y_true) == 0:\n            return {'accuracy':np.nan, 'f1_score':np.nan, 'roc_auc':np.nan, 'sensitivity':np.nan, 'specificity':np.nan}\n        preds = (y_probs > threshold).astype(int)\n        m = {}\n        m['accuracy'] = float(accuracy_score(y_true, preds))\n        m['f1_score'] = float(f1_score(y_true, preds, zero_division=0))\n        # Sensitivity (Recall)\n        m['sensitivity'] = float(recall_score(y_true, preds, zero_division=0))\n        # Specificity\n        tn, fp, fn, tp = confusion_matrix(y_true, preds, labels=[0,1]).ravel()\n        m['specificity'] = float(tn / (tn + fp)) if (tn + fp) > 0 else 0.0\n        # AUC\n        if len(np.unique(y_true)) > 1:\n            m['roc_auc'] = float(roc_auc_score(y_true, y_probs))\n        else:\n            m['roc_auc'] = np.nan\n        return m\n\n    # Calculate metrics for groups\n    results = {}\n    results['Overall'] = compute_metrics(all_labels, all_probs)\n\n    # Gender Groups\n    results['Male'] = compute_metrics(all_labels[all_genders == 0], all_probs[all_genders == 0])\n    results['Female'] = compute_metrics(all_labels[all_genders == 1], all_probs[all_genders == 1])\n\n    # Race Groups (0=White, 1=Black, 2=Asian)\n    results['White'] = compute_metrics(all_labels[all_races == 0], all_probs[all_races == 0])\n    results['Black'] = compute_metrics(all_labels[all_races == 1], all_probs[all_races == 1])\n    results['Asian'] = compute_metrics(all_labels[all_races == 2], all_probs[all_races == 2])\n\n    return results, all_labels, all_probs, all_genders\n\n# -------------------------\n# Training Loop\n# -------------------------\ndef train_full():\n    # Setup\n    if not (os.path.exists(TRAIN_DIR) and os.path.exists(TEST_DIR)):\n        download_and_extract_fairdomain()\n    else:\n        print(\"Training/testing directories already present.\")\n\n    # Load summary CSV for demographics\n    summary_map = load_summary_csv_as_map(SUMMARY_CSV_PATH)\n\n    print(\"Loading data...\")\n    training_data = load_npz_dir(TRAIN_DIR)\n    testing_data = load_npz_dir(TEST_DIR)\n\n    oct_train_arr, slo_train_arr, labels_train_arr = process_dataset_data_to_arrays(training_data, summary_map)\n    oct_test_arr, slo_test_arr, labels_test_arr = process_dataset_data_to_arrays(testing_data, summary_map)\n    print(\"Train size:\", len(oct_train_arr), \" Test size:\", len(oct_test_arr))\n\n    if len(oct_train_arr) == 0:\n        raise RuntimeError(\"No training data.\")\n\n    # Data Loaders\n    input_size = 224 # ResNet default\n\n    try:\n        randaug = transforms.RandAugment(num_ops=2, magnitude=9)\n    except:\n        randaug = None\n\n    train_transform = transforms.Compose([\n        transforms.Resize((input_size,input_size)),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomRotation(10),\n        transforms.RandomResizedCrop(input_size, scale=(0.85,1.0)),\n        randaug if randaug is not None else transforms.Identity(),\n        transforms.ColorJitter(brightness=0.12, contrast=0.12, saturation=0.06),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[INPUT_MEAN], std=[INPUT_STD])\n    ])\n    test_transform = transforms.Compose([\n        transforms.Resize((input_size,input_size)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[INPUT_MEAN], std=[INPUT_STD])\n    ])\n\n    train_ds = DualImageDataset(oct_train_arr, slo_train_arr, labels_train_arr, transform=train_transform)\n    test_ds = DualImageDataset(oct_test_arr, slo_test_arr, labels_test_arr, transform=test_transform)\n\n    # Weighted Sampler\n    glaucoma_labels = labels_train_arr[:,0]\n    class_counts = np.bincount(glaucoma_labels.astype(int))\n    print(\"Class counts:\", class_counts)\n    weights = 1.0 / (class_counts[glaucoma_labels.astype(int)] + 1e-6)\n    sampler = WeightedRandomSampler(torch.from_numpy(weights), len(weights))\n\n    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=sampler, num_workers=2, pin_memory=True)\n    test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n\n    # Init Model\n    model = BaselineResNet34(device=DEVICE)\n    model.to(DEVICE)\n\n    # Calculate pos_weight for BCE\n    pos_c = (glaucoma_labels == 1).sum()\n    neg_c = (glaucoma_labels == 0).sum()\n    pw_val = float(neg_c) / (float(pos_c) + 1e-8)\n    model.pos_weight = torch.tensor(pw_val, device=DEVICE)\n    print(f\"Using pos_weight: {pw_val:.2f}\")\n\n    # --- Stage 1: Freeze Backbone, Train MLP ---\n    print(\"\\n--- Stage 1: Training MLP Head ---\")\n    for p in model.backbone.parameters(): p.requires_grad = False\n    for p in model.classifier.parameters(): p.requires_grad = True\n\n    opt1 = optim.AdamW(model.classifier.parameters(), lr=HEAD_LR, weight_decay=WEIGHT_DECAY)\n    scaler = torch.amp.GradScaler()\n\n    best_loss = float('inf')\n    best_wts = copy.deepcopy(model.state_dict())\n    history = {\"train_loss\": []}\n\n    def train_epoch(optimizer):\n        model.train()\n        run_loss = 0; count = 0\n        for oct_img, slo_img, lbls in train_loader:\n            oct_img = oct_img.to(DEVICE); slo_img = slo_img.to(DEVICE); lbls = lbls.to(DEVICE)\n            optimizer.zero_grad()\n\n            # Mixup\n            if USE_MIXUP:\n                mx1, mx2, ya, yb, _, lam = mixup_data(oct_img, slo_img, lbls, MIXUP_ALPHA, DEVICE)\n                with torch.amp.autocast(device_type=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n                    logits = model(mx1, mx2)\n                    loss_fn = nn.BCEWithLogitsLoss(pos_weight=model.pos_weight)\n                    loss = lam * loss_fn(logits, ya[:,0:1]) + (1 - lam) * loss_fn(logits, yb[:,0:1])\n            else:\n                with torch.amp.autocast(device_type=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n                    logits = model(oct_img, slo_img)\n                    loss_fn = nn.BCEWithLogitsLoss(pos_weight=model.pos_weight)\n                    loss = loss_fn(logits, lbls[:,0:1])\n\n            scaler.scale(loss).backward()\n            scaler.step(optimizer); scaler.update()\n            run_loss += loss.item(); count += 1\n        return run_loss / max(1, count)\n\n    for e in range(NUM_EPOCHS_STAGE1):\n        loss = train_epoch(opt1)\n        print(f\"Ep {e+1}: Loss {loss:.4f}\")\n        history['train_loss'].append(loss)\n        if loss < best_loss:\n            best_loss = loss; best_wts = copy.deepcopy(model.state_dict())\n\n    # --- Stage 2: Fine-tune All ---\n    print(\"\\n--- Stage 2: Fine-tuning All ---\")\n    for p in model.parameters(): p.requires_grad = True\n\n    # Differential learning rates\n    opt2 = optim.AdamW([\n        {'params': model.backbone.parameters(), 'lr': BACKBONE_LR},\n        {'params': model.classifier.parameters(), 'lr': HEAD_LR}\n    ], weight_decay=WEIGHT_DECAY)\n\n    # Scheduler\n    steps = len(train_loader) * NUM_EPOCHS_STAGE2\n    sched = torch.optim.lr_scheduler.OneCycleLR(opt2, max_lr=HEAD_LR, total_steps=steps)\n\n    # SWA Setup\n    swa_model = None; swa_sched = None\n    if USE_SWA:\n        from torch.optim.swa_utils import AveragedModel, SWALR\n        swa_model = AveragedModel(model)\n        swa_sched = SWALR(opt2, swa_lr=1e-4)\n\n    no_imp = 0\n    for e in range(NUM_EPOCHS_STAGE2):\n        loss = train_epoch(opt2)\n        sched.step()\n\n        # SWA Update\n        if swa_model and (e+1) >= SWA_START_EPOCH:\n            swa_model.update_parameters(model)\n            swa_sched.step()\n\n        print(f\"Ep {e+1}: Loss {loss:.4f}\")\n        history['train_loss'].append(loss)\n\n        if loss < best_loss - 1e-4:\n            best_loss = loss; best_wts = copy.deepcopy(model.state_dict())\n            no_imp = 0\n        else:\n            no_imp += 1\n            if no_imp >= EARLY_STOP_PATIENCE:\n                print(\"Early stopping.\"); break\n\n    # Finalize\n    if swa_model:\n        # Use CUSTOM update_bn to avoid \"missing argument\" error\n        print(\"Finalizing SWA...\")\n        model.load_state_dict(swa_model.module.state_dict())\n        custom_update_bn(train_loader, model, device=DEVICE)\n    else:\n        model.load_state_dict(best_wts)\n\n    # Evaluation\n    print(\"\\n--- Final Evaluation ---\")\n    metrics, _, _, _ = evaluate_with_demographics(model, test_loader, DEVICE)\n\n    for k, v in metrics.items():\n        print(f\"\\n{k}:\")\n        for mk, mv in v.items():\n            print(f\"  {mk}: {mv:.4f}\")\n\n    return model, metrics, history\n\nif __name__ == \"__main__\":\n    train_full()","metadata":{"id":"6ZazY3Njoug3","colab":{"base_uri":"https://localhost:8080/"},"outputId":"46b17f9f-a206-4789-8a70-0c0a5c6a3c4d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training/testing directories already present.\n","Loading data...\n","Train size: 8000  Test size: 2000\n","Class counts: [4434 3566]\n","Using pos_weight: 1.24\n","\n","--- Stage 1: Training MLP Head ---\n","Ep 1: Loss 0.7239\n","Ep 2: Loss 0.7027\n","Ep 3: Loss 0.6934\n","Ep 4: Loss 0.6936\n","Ep 5: Loss 0.7012\n","Ep 6: Loss 0.6917\n","Ep 7: Loss 0.6913\n","Ep 8: Loss 0.6888\n","Ep 9: Loss 0.6916\n","Ep 10: Loss 0.6877\n","\n","--- Stage 2: Fine-tuning All ---\n","Ep 1: Loss 0.6751\n","Ep 2: Loss 0.6455\n","Ep 3: Loss 0.6280\n","Ep 4: Loss 0.6037\n","Ep 5: Loss 0.5984\n","Ep 6: Loss 0.5800\n","Ep 7: Loss 0.5695\n","Ep 8: Loss 0.5554\n","Ep 9: Loss 0.5458\n","Ep 10: Loss 0.5263\n","Ep 11: Loss 0.5245\n","Ep 12: Loss 0.5172\n","Ep 13: Loss 0.4976\n","Ep 14: Loss 0.4989\n","Ep 15: Loss 0.4891\n","Ep 16: Loss 0.4834\n","Ep 17: Loss 0.4635\n","Ep 18: Loss 0.4670\n","Ep 19: Loss 0.4487\n","Ep 20: Loss 0.4210\n","Ep 21: Loss 0.4084\n","Ep 22: Loss 0.4277\n","Ep 23: Loss 0.4267\n","Ep 24: Loss 0.3878\n","Ep 25: Loss 0.4041\n","Ep 26: Loss 0.3941\n","Ep 27: Loss 0.4141\n","Ep 28: Loss 0.3751\n","Ep 29: Loss 0.3785\n","Ep 30: Loss 0.3753\n","Ep 31: Loss 0.3683\n","Ep 32: Loss 0.3805\n","Ep 33: Loss 0.3608\n","Ep 34: Loss 0.3486\n","Ep 35: Loss 0.3434\n","Ep 36: Loss 0.3265\n","Ep 37: Loss 0.3640\n","Ep 38: Loss 0.3394\n","Ep 39: Loss 0.3580\n","Ep 40: Loss 0.3635\n","Ep 41: Loss 0.3516\n","Ep 42: Loss 0.3424\n","Ep 43: Loss 0.3194\n","Ep 44: Loss 0.3569\n","Ep 45: Loss 0.3118\n","Ep 46: Loss 0.3480\n","Ep 47: Loss 0.3078\n","Ep 48: Loss 0.3082\n","Ep 49: Loss 0.3452\n","Ep 50: Loss 0.3423\n","Finalizing SWA...\n","\n","--- Final Evaluation ---\n","\n","Overall:\n","  accuracy: 0.7675\n","  f1_score: 0.6986\n","  sensitivity: 0.6077\n","  specificity: 0.8949\n","  roc_auc: 0.8454\n","\n","Male:\n","  accuracy: 0.7804\n","  f1_score: 0.7153\n","  sensitivity: 0.6333\n","  specificity: 0.8939\n","  roc_auc: 0.8460\n","\n","Female:\n","  accuracy: 0.7626\n","  f1_score: 0.6923\n","  sensitivity: 0.5981\n","  specificity: 0.8953\n","  roc_auc: 0.8452\n","\n","White:\n","  accuracy: 0.7806\n","  f1_score: 0.6907\n","  sensitivity: 0.6085\n","  specificity: 0.8966\n","  roc_auc: 0.8469\n","\n","Black:\n","  accuracy: 0.7043\n","  f1_score: 0.7262\n","  sensitivity: 0.6146\n","  specificity: 0.8624\n","  roc_auc: 0.8291\n","\n","Asian:\n","  accuracy: 0.7627\n","  f1_score: 0.6957\n","  sensitivity: 0.5854\n","  specificity: 0.9158\n","  roc_auc: 0.8389\n"]}],"execution_count":null},{"cell_type":"markdown","source":"# Dual EfficientNetB3 encoder + MLP","metadata":{"id":"bFUo6UP7VLgg"}},{"cell_type":"code","source":"# combined_dual_effnet_mlp_metrics_fix.py\n# Architecture: Dual EfficientNet-B3 Encoders + MLP Classifier\n# Fix: Added custom_update_bn to handle SWA with two input images\n\nimport os, sys, zipfile, glob, subprocess, copy, math, time, traceback\nfrom typing import Tuple, Dict, Any\n\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport torchvision.models as models\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n\nimport matplotlib.pyplot as plt\n\n# -------------------------\n# Config\n# -------------------------\nGDRIVE_LINK = \"Drive link to the fairdomain zip file\"\nFAIRDOMAIN_ZIP = \"FairDomain.zip\"\nEXTRACT_ROOT = \"dataset_extracted\"\nTRAIN_DIR = os.path.join(EXTRACT_ROOT, \"Training\")\nTEST_DIR  = os.path.join(EXTRACT_ROOT, \"Testing\")\nSUMMARY_CSV_PATH = \"data_summary.csv\"\n\n# Model/training hyperparameters\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nSEED = 42\nBATCH_SIZE = 16  # Reduced slightly for Dual EffNetB3 memory usage\nNUM_EPOCHS_STAGE1 = 10\nNUM_EPOCHS_STAGE2 = 50\nHEAD_LR = 1e-3\nBACKBONE_LR = 3e-5\nWEIGHT_DECAY = 1e-5\nINPUT_MEAN = 0.5\nINPUT_STD  = 0.25\n\nGLAUCOMA_MLP_HIDDEN = 256\nMAX_GRAD_NORM = 1.0\nEARLY_STOP_PATIENCE = 12\n\nUSE_MIXUP = True\nMIXUP_ALPHA = 0.2\nUSE_SWA = True\nSWA_START_EPOCH = max(1, NUM_EPOCHS_STAGE2 - 5)\n# -------------------------\n\ndef run_shell(cmd):\n    print(\"RUN:\", cmd)\n    r = subprocess.run(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    if r.returncode != 0:\n        print(r.stderr.decode(\"utf-8\"))\n    return r\n\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\n# -------------------------\n# Data Setup Helpers\n# -------------------------\ndef download_and_extract_fairdomain(gdrive_link=GDRIVE_LINK, out_zip=FAIRDOMAIN_ZIP):\n    try:\n        import gdown\n    except Exception:\n        print(\"Installing gdown...\")\n        run_shell(\"pip install -q gdown\")\n        import gdown\n    parts = gdrive_link.split(\"/\")\n    file_id = None\n    for i,p in enumerate(parts):\n        if p == \"d\" and i+1 < len(parts):\n            file_id = parts[i+1]; break\n    if file_id is None:\n        file_id = gdrive_link.split(\"/\")[-2]\n    print(\"Downloading FairDomain.zip (id=%s)...\" % file_id)\n    gdown.download(id=file_id, output=out_zip, quiet=False)\n    if not os.path.exists(out_zip):\n        raise RuntimeError(\"Download failed: %s\" % out_zip)\n    print(\"Extracting FairDomain.zip ...\")\n    with zipfile.ZipFile(out_zip, \"r\") as z:\n        z.extractall(\".\")\n    candidates = glob.glob(\"**/dataset*.zip\", recursive=True)\n    if not candidates:\n        candidates = glob.glob(\"**/dataset.zip\", recursive=True)\n    if not candidates:\n        raise RuntimeError(\"Could not find dataset.zip inside FairDomain.zip extracted contents.\")\n    dataset_zip = candidates[0]\n    print(\"Found dataset:\", dataset_zip)\n    os.makedirs(EXTRACT_ROOT, exist_ok=True)\n    with zipfile.ZipFile(dataset_zip, \"r\") as z:\n        z.extractall(EXTRACT_ROOT)\n    print(\"Dataset extracted to\", EXTRACT_ROOT)\n\ndef normalize_race_value(rv) -> int:\n    \"\"\" Maps race inputs to integers: 0=White, 1=Black, 2=Asian, 3=Other \"\"\"\n    if rv is None: return 3\n    try:\n        if isinstance(rv, (str, bytes, bytearray)):\n             s = str(rv).strip().lower()\n             if 'white' in s: return 0\n             if 'black' in s or 'african' in s: return 1\n             if 'asian' in s: return 2\n             return 3\n        if np.isscalar(rv):\n            if int(rv) in (0,1,2,3): return int(rv)\n            return 3\n        return 3\n    except Exception:\n        return 3\n\ndef load_summary_csv_as_map(csv_path: str) -> Dict[str, Dict[str, Any]]:\n    if not os.path.exists(csv_path):\n        print(f\"Warning: Summary CSV not found at {csv_path}\")\n        return {}\n    try:\n        df = pd.read_csv(csv_path)\n    except Exception as e:\n        print(f\"Error reading CSV: {e}\")\n        return {}\n\n    possible_id_cols = [c for c in df.columns if c.lower() in ('id','subject_id','patient_id','filename','file','case','name')]\n    chosen_id_col = possible_id_cols[0] if possible_id_cols else None\n\n    if chosen_id_col is None: return {}\n\n    df['_key'] = df[chosen_id_col].astype(str).str.strip()\n    mapping = {}\n\n    for _, row in df.iterrows():\n        key = row['_key']\n        key_clean = os.path.splitext(key)[0]\n        mapping[key] = row.to_dict()\n        mapping[key_clean] = row.to_dict()\n\n    return mapping\n\ndef load_npz_dir(root_dir):\n    out = {}\n    if not os.path.exists(root_dir):\n        print(\"Warning: path does not exist:\", root_dir)\n        return out\n    for root, _, files in os.walk(root_dir):\n        for f in files:\n            if f.endswith(\".npz\"):\n                p = os.path.join(root, f)\n                k = os.path.splitext(f)[0]\n                try:\n                    data = np.load(p, allow_pickle=True)\n                    out[k] = dict(data)\n                except Exception as e:\n                    print(\"Failed to load\", p, e)\n    return out\n\ndef process_dataset_data_to_arrays(data_dict, summary_map: Dict[str, Dict[str, Any]] = None):\n    octs, slos, labels = [], [], []\n    keys = sorted(data_dict.keys())\n    for k in keys:\n        d = data_dict[k]\n        if \"oct_fundus\" not in d or \"slo_fundus\" not in d:\n            continue\n        o = d[\"oct_fundus\"]; s = d[\"slo_fundus\"]\n        if o.ndim == 3 and o.shape[-1] == 1: o = np.squeeze(o, axis=-1)\n        if s.ndim == 3 and s.shape[-1] == 1: s = np.squeeze(s, axis=-1)\n\n        glaucoma = float(d.get(\"glaucoma\", 0.0))\n        gender = float(d.get(\"gender\", 0.0))\n        age = float(d.get(\"age\", 0.0))\n        md = float(d.get(\"md\", 0.0))\n        race_val = d.get(\"race\", None)\n\n        if summary_map and k in summary_map:\n            entry = summary_map[k]\n            if 'glaucoma' in entry and pd.notna(entry['glaucoma']):\n                g_val = str(entry['glaucoma']).lower()\n                glaucoma = 1.0 if g_val in ['yes', '1', 'true'] else 0.0\n            if 'gender' in entry and pd.notna(entry['gender']):\n                g_val = str(entry['gender']).lower()\n                gender = 1.0 if g_val in ['female', 'f', '1'] else 0.0\n            if 'race' in entry and pd.notna(entry['race']):\n                race_val = entry['race']\n            if 'age' in entry and pd.notna(entry['age']):\n                try: age = float(entry['age'])\n                except: pass\n            if 'md' in entry and pd.notna(entry['md']):\n                try: md = float(entry['md'])\n                except: pass\n\n        race_code = normalize_race_value(race_val)\n\n        octs.append(o.astype(np.float32)); slos.append(s.astype(np.float32))\n        labels.append([glaucoma, gender, age, md, float(race_code)])\n\n    if len(octs) == 0: return np.array([]), np.array([]), np.array([])\n    return np.array(octs), np.array(slos), np.array(labels, dtype=np.float32)\n\nclass DualImageDataset(Dataset):\n    def __init__(self, oct_images, slo_images, labels, transform=None):\n        assert len(oct_images) == len(slo_images) == len(labels)\n        self.oct_images = oct_images; self.slo_images = slo_images; self.labels = labels; self.transform = transform\n    def _to_pil(self,a):\n        arr = a\n        if arr.dtype != np.uint8:\n            if arr.max() <= 1.0: arr = (arr*255.0).astype(np.uint8)\n            else: arr = arr.astype(np.uint8)\n        return Image.fromarray(arr)\n    def __len__(self): return len(self.oct_images)\n    def __getitem__(self, idx):\n        o = self.oct_images[idx]; s = self.slo_images[idx]\n        o_pil = self._to_pil(o); s_pil = self._to_pil(s)\n        if self.transform:\n            o_t = self.transform(o_pil); s_t = self.transform(s_pil)\n        else:\n            o_t = transforms.ToTensor()(o_pil); s_t = transforms.ToTensor()(s_pil)\n        lbl = torch.tensor(self.labels[idx], dtype=torch.float32)\n        return o_t, s_t, lbl\n\n# -------------------------\n# Custom Update BN for SWA\n# -------------------------\ndef custom_update_bn(loader, model, device=None):\n    \"\"\"\n    Custom update_bn that handles dataloader returning (oct, slo, label).\n    Standard torch.optim.swa_utils.update_bn expects (input, label)\n    and passes input directly to model(input).\n    \"\"\"\n    if device is None:\n        device = next(model.parameters()).device\n\n    momenta = {}\n    for module in model.modules():\n        if isinstance(module, torch.nn.modules.batchnorm._BatchNorm):\n            module.running_mean = torch.zeros_like(module.running_mean)\n            module.running_var = torch.ones_like(module.running_var)\n            momenta[module] = module.momentum\n            module.momentum = None\n            module.num_batches_tracked *= 0\n\n    model.train()\n    n = 0\n    with torch.no_grad():\n        for oct_img, slo_img, _ in loader:\n            oct_img = oct_img.to(device)\n            slo_img = slo_img.to(device)\n            # Forward pass to update BN stats\n            model(oct_img, slo_img)\n            n += 1\n\n    for module in model.modules():\n        if isinstance(module, torch.nn.modules.batchnorm._BatchNorm):\n            module.momentum = momenta[module]\n\n# -------------------------\n# Architecture: Dual EffNetB3 + MLP\n# -------------------------\nclass GlaucomaPredictorMLP(nn.Module):\n    def __init__(self, input_dim, hidden_dim=GLAUCOMA_MLP_HIDDEN, dropout=0.4):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.BatchNorm1d(hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, max(8, hidden_dim//2)),\n            nn.BatchNorm1d(max(8, hidden_dim//2)),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout/2),\n            nn.Linear(max(8, hidden_dim//2), 1)\n        )\n    def forward(self,x): return self.net(x)\n\nclass SingleEncoderGeneric(nn.Module):\n    def __init__(self, backbone='efficientnet_b3', pretrained=True):\n        super().__init__()\n        # Load EffNetB3\n        try:\n            from torchvision.models import EfficientNet_B3_Weights\n            weights = EfficientNet_B3_Weights.IMAGENET1K_V1 if pretrained else None\n            net = models.efficientnet_b3(weights=weights)\n        except Exception:\n            net = models.efficientnet_b3(pretrained=bool(pretrained))\n\n        # Modify first layer for 1 channel\n        first_conv = net.features[0][0]\n        if isinstance(first_conv, nn.Conv2d):\n            new_conv = nn.Conv2d(1, first_conv.out_channels,\n                               kernel_size=first_conv.kernel_size,\n                               stride=first_conv.stride,\n                               padding=first_conv.padding,\n                               bias=(first_conv.bias is not None))\n            with torch.no_grad():\n                new_conv.weight[:] = first_conv.weight.mean(dim=1, keepdim=True)\n            net.features[0][0] = new_conv\n\n        self.backbone = net.features\n        self.pool = nn.AdaptiveAvgPool2d((1,1))\n\n        # Determine feature dim\n        with torch.no_grad():\n            dummy = torch.randn(1, 1, 224, 224)\n            out = self.backbone(dummy)\n            self.feat_dim = out.shape[1] # Should be 1536 for EffNetB3\n\n    def forward(self, x):\n        if x.dim() == 3: x = x.unsqueeze(1)\n        f = self.backbone(x)\n        f = self.pool(f).view(f.size(0), -1)\n        return f\n\nclass DualEfficientNetClassifier(nn.Module):\n    def __init__(self, device=DEVICE):\n        super().__init__()\n        # Two separate encoders\n        self.enc_o = SingleEncoderGeneric(backbone='efficientnet_b3', pretrained=True)\n        self.enc_s = SingleEncoderGeneric(backbone='efficientnet_b3', pretrained=True)\n\n        # Concatenated features dimension\n        self.feat_dim = self.enc_o.feat_dim + self.enc_s.feat_dim\n\n        # MLP Classifier\n        self.classifier = GlaucomaPredictorMLP(input_dim=self.feat_dim, hidden_dim=GLAUCOMA_MLP_HIDDEN)\n\n        self.pos_weight = None\n\n    def forward(self, oct_img, slo_img):\n        f_oct = self.enc_o(oct_img)\n        f_slo = self.enc_s(slo_img)\n\n        combined = torch.cat([f_oct, f_slo], dim=1)\n        logits = self.classifier(combined)\n        return logits\n\n# -------------------------\n# Training & Eval Logic\n# -------------------------\ndef mixup_data(x1, x2, y, alpha=0.2, device=DEVICE):\n    if alpha <= 0:\n        return x1, x2, y, None, None, 1.0\n    lam = np.random.beta(alpha, alpha)\n    batch_size = x1.size(0)\n    index = torch.randperm(batch_size).to(device)\n    mixed_x1 = lam * x1 + (1 - lam) * x1[index, :]\n    mixed_x2 = lam * x2 + (1 - lam) * x2[index, :]\n    y_a, y_b = y, y[index]\n    return mixed_x1, mixed_x2, y_a, y_b, index, lam\n\ndef evaluate_with_demographics(model, dataloader, device, threshold=0.5):\n    model.eval()\n    all_probs, all_labels, all_genders, all_races = [], [], [], []\n    with torch.no_grad():\n        for oct_img, slo_img, lbls in dataloader:\n            oct_img = oct_img.to(device); slo_img = slo_img.to(device); lbls = lbls.to(device)\n            glaucoma_label_true = lbls[:,0].view(-1).cpu().numpy()\n            genders = lbls[:,1].view(-1).cpu().numpy()\n            races   = lbls[:,4].view(-1).cpu().numpy()\n\n            logits = model(oct_img, slo_img)\n            probs = torch.sigmoid(logits).view(-1).cpu().numpy()\n\n            all_probs.append(probs); all_labels.append(glaucoma_label_true)\n            all_genders.append(genders); all_races.append(races)\n\n    if len(all_probs) == 0:\n        return {}, np.array([]), np.array([]), np.array([])\n    all_probs = np.concatenate(all_probs)\n    all_labels = np.concatenate(all_labels).astype(int)\n    all_genders = np.concatenate(all_genders).astype(int)\n    all_races = np.concatenate(all_races).astype(int)\n\n    def compute_metrics(y_true, y_probs):\n        if len(y_true) == 0:\n            return {'accuracy':np.nan, 'f1_score':np.nan, 'roc_auc':np.nan, 'sensitivity':np.nan, 'specificity':np.nan}\n        preds = (y_probs > threshold).astype(int)\n        m = {}\n        m['accuracy'] = float(accuracy_score(y_true, preds))\n        m['f1_score'] = float(f1_score(y_true, preds, zero_division=0))\n        m['sensitivity'] = float(recall_score(y_true, preds, zero_division=0))\n        tn, fp, fn, tp = confusion_matrix(y_true, preds, labels=[0,1]).ravel()\n        m['specificity'] = float(tn / (tn + fp)) if (tn + fp) > 0 else 0.0\n        if len(np.unique(y_true)) > 1:\n            m['roc_auc'] = float(roc_auc_score(y_true, y_probs))\n        else:\n            m['roc_auc'] = np.nan\n        return m\n\n    results = {}\n    results['Overall'] = compute_metrics(all_labels, all_probs)\n    results['Male'] = compute_metrics(all_labels[all_genders == 0], all_probs[all_genders == 0])\n    results['Female'] = compute_metrics(all_labels[all_genders == 1], all_probs[all_genders == 1])\n    results['White'] = compute_metrics(all_labels[all_races == 0], all_probs[all_races == 0])\n    results['Black'] = compute_metrics(all_labels[all_races == 1], all_probs[all_races == 1])\n    results['Asian'] = compute_metrics(all_labels[all_races == 2], all_probs[all_races == 2])\n\n    return results, all_labels, all_probs, all_genders\n\ndef train_full():\n    if not (os.path.exists(TRAIN_DIR) and os.path.exists(TEST_DIR)):\n        download_and_extract_fairdomain()\n    else:\n        print(\"Training/testing directories already present.\")\n\n    summary_map = load_summary_csv_as_map(SUMMARY_CSV_PATH)\n\n    print(\"Loading data...\")\n    training_data = load_npz_dir(TRAIN_DIR)\n    testing_data = load_npz_dir(TEST_DIR)\n\n    oct_train_arr, slo_train_arr, labels_train_arr = process_dataset_data_to_arrays(training_data, summary_map)\n    oct_test_arr, slo_test_arr, labels_test_arr = process_dataset_data_to_arrays(testing_data, summary_map)\n    print(\"Train size:\", len(oct_train_arr), \" Test size:\", len(oct_test_arr))\n\n    if len(oct_train_arr) == 0:\n        raise RuntimeError(\"No training data.\")\n\n    input_size = 300 # EffNetB3 default\n    try:\n        randaug = transforms.RandAugment(num_ops=2, magnitude=9)\n    except:\n        randaug = None\n\n    train_transform = transforms.Compose([\n        transforms.Resize((input_size,input_size)),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomRotation(10),\n        transforms.RandomResizedCrop(input_size, scale=(0.85,1.0)),\n        randaug if randaug is not None else transforms.Identity(),\n        transforms.ColorJitter(brightness=0.12, contrast=0.12, saturation=0.06),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[INPUT_MEAN], std=[INPUT_STD])\n    ])\n    test_transform = transforms.Compose([\n        transforms.Resize((input_size,input_size)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[INPUT_MEAN], std=[INPUT_STD])\n    ])\n\n    train_ds = DualImageDataset(oct_train_arr, slo_train_arr, labels_train_arr, transform=train_transform)\n    test_ds = DualImageDataset(oct_test_arr, slo_test_arr, labels_test_arr, transform=test_transform)\n\n    glaucoma_labels = labels_train_arr[:,0]\n    class_counts = np.bincount(glaucoma_labels.astype(int))\n    print(\"Class counts:\", class_counts)\n    weights = 1.0 / (class_counts[glaucoma_labels.astype(int)] + 1e-6)\n    sampler = WeightedRandomSampler(torch.from_numpy(weights), len(weights))\n\n    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=sampler, num_workers=2, pin_memory=True)\n    test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n\n    # Init Model\n    model = DualEfficientNetClassifier(device=DEVICE)\n    model.to(DEVICE)\n\n    pos_c = (glaucoma_labels == 1).sum()\n    neg_c = (glaucoma_labels == 0).sum()\n    pw_val = float(neg_c) / (float(pos_c) + 1e-8)\n    model.pos_weight = torch.tensor(pw_val, device=DEVICE)\n    print(f\"Using pos_weight: {pw_val:.2f}\")\n\n    # --- Stage 1: Freeze Backbone, Train MLP ---\n    print(\"\\n--- Stage 1: Training MLP Head ---\")\n    for p in model.enc_o.parameters(): p.requires_grad = False\n    for p in model.enc_s.parameters(): p.requires_grad = False\n    for p in model.classifier.parameters(): p.requires_grad = True\n\n    opt1 = optim.AdamW(model.classifier.parameters(), lr=HEAD_LR, weight_decay=WEIGHT_DECAY)\n    scaler = torch.amp.GradScaler()\n\n    best_loss = float('inf')\n    best_wts = copy.deepcopy(model.state_dict())\n    history = {\"train_loss\": []}\n\n    def train_epoch(optimizer):\n        model.train()\n        run_loss = 0; count = 0\n        for oct_img, slo_img, lbls in train_loader:\n            oct_img = oct_img.to(DEVICE); slo_img = slo_img.to(DEVICE); lbls = lbls.to(DEVICE)\n            optimizer.zero_grad()\n\n            if USE_MIXUP:\n                mx1, mx2, ya, yb, _, lam = mixup_data(oct_img, slo_img, lbls, MIXUP_ALPHA, DEVICE)\n                with torch.amp.autocast(device_type=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n                    logits = model(mx1, mx2)\n                    loss_fn = nn.BCEWithLogitsLoss(pos_weight=model.pos_weight)\n                    loss = lam * loss_fn(logits, ya[:,0:1]) + (1 - lam) * loss_fn(logits, yb[:,0:1])\n            else:\n                with torch.amp.autocast(device_type=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n                    logits = model(oct_img, slo_img)\n                    loss_fn = nn.BCEWithLogitsLoss(pos_weight=model.pos_weight)\n                    loss = loss_fn(logits, lbls[:,0:1])\n\n            scaler.scale(loss).backward()\n            scaler.step(optimizer); scaler.update()\n            run_loss += loss.item(); count += 1\n        return run_loss / max(1, count)\n\n    for e in range(NUM_EPOCHS_STAGE1):\n        loss = train_epoch(opt1)\n        print(f\"Ep {e+1}: Loss {loss:.4f}\")\n        history['train_loss'].append(loss)\n        if loss < best_loss:\n            best_loss = loss; best_wts = copy.deepcopy(model.state_dict())\n\n    # --- Stage 2: Fine-tune All ---\n    print(\"\\n--- Stage 2: Fine-tuning All ---\")\n    for p in model.parameters(): p.requires_grad = True\n\n    opt2 = optim.AdamW([\n        {'params': list(model.enc_o.parameters()) + list(model.enc_s.parameters()), 'lr': BACKBONE_LR},\n        {'params': model.classifier.parameters(), 'lr': HEAD_LR}\n    ], weight_decay=WEIGHT_DECAY)\n\n    steps = len(train_loader) * NUM_EPOCHS_STAGE2\n    sched = torch.optim.lr_scheduler.OneCycleLR(opt2, max_lr=HEAD_LR, total_steps=steps)\n\n    swa_model = None; swa_sched = None\n    if USE_SWA:\n        from torch.optim.swa_utils import AveragedModel, SWALR\n        swa_model = AveragedModel(model)\n        swa_sched = SWALR(opt2, swa_lr=1e-4)\n\n    no_imp = 0\n    for e in range(NUM_EPOCHS_STAGE2):\n        loss = train_epoch(opt2)\n        sched.step()\n\n        if swa_model and (e+1) >= SWA_START_EPOCH:\n            swa_model.update_parameters(model)\n            swa_sched.step()\n\n        print(f\"Ep {e+1}: Loss {loss:.4f}\")\n        history['train_loss'].append(loss)\n\n        if loss < best_loss - 1e-4:\n            best_loss = loss; best_wts = copy.deepcopy(model.state_dict())\n            no_imp = 0\n        else:\n            no_imp += 1\n            if no_imp >= EARLY_STOP_PATIENCE:\n                print(\"Early stopping.\"); break\n\n    if swa_model:\n        print(\"Finalizing SWA...\")\n        model.load_state_dict(swa_model.module.state_dict())\n        custom_update_bn(train_loader, model, device=DEVICE)\n    else:\n        model.load_state_dict(best_wts)\n\n    print(\"\\n--- Final Evaluation ---\")\n    metrics, _, _, _ = evaluate_with_demographics(model, test_loader, DEVICE)\n\n    for k, v in metrics.items():\n        print(f\"\\n{k}:\")\n        for mk, mv in v.items():\n            print(f\"  {mk}: {mv:.4f}\")\n\n    return model, metrics, history\n\nif __name__ == \"__main__\":\n    train_full()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PoIMoAEUVQ0e","outputId":"a0738eb3-0172-4e32-be43-93f85fde792e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training/testing directories already present.\n","Loading data...\n","Train size: 8000  Test size: 2000\n","Class counts: [4434 3566]\n","Using pos_weight: 1.24\n","\n","--- Stage 1: Training MLP Head ---\n","Ep 1: Loss 0.7129\n","Ep 2: Loss 0.6927\n","Ep 3: Loss 0.6964\n","Ep 4: Loss 0.6867\n","Ep 5: Loss 0.6741\n","Ep 6: Loss 0.6774\n","Ep 7: Loss 0.6832\n","Ep 8: Loss 0.6788\n","Ep 9: Loss 0.6919\n","Ep 10: Loss 0.6832\n","\n","--- Stage 2: Fine-tuning All ---\n","Ep 1: Loss 0.6640\n","Ep 2: Loss 0.6507\n","Ep 3: Loss 0.6328\n","Ep 4: Loss 0.6237\n","Ep 5: Loss 0.6095\n","Ep 6: Loss 0.6010\n","Ep 7: Loss 0.5940\n","Ep 8: Loss 0.5761\n","Ep 9: Loss 0.5578\n","Ep 10: Loss 0.5420\n","Ep 11: Loss 0.5369\n","Ep 12: Loss 0.5258\n","Ep 13: Loss 0.4996\n","Ep 14: Loss 0.5036\n","Ep 15: Loss 0.4869\n","Ep 16: Loss 0.4641\n","Ep 17: Loss 0.4630\n","Ep 18: Loss 0.4495\n","Ep 19: Loss 0.4480\n","Ep 20: Loss 0.4198\n","Ep 21: Loss 0.4031\n","Ep 22: Loss 0.4083\n","Ep 23: Loss 0.4042\n","Ep 24: Loss 0.4012\n","Ep 25: Loss 0.3767\n","Ep 26: Loss 0.3992\n","Ep 27: Loss 0.3833\n","Ep 28: Loss 0.3412\n","Ep 29: Loss 0.3355\n","Ep 30: Loss 0.3633\n","Ep 31: Loss 0.3751\n","Ep 32: Loss 0.3446\n","Ep 33: Loss 0.3509\n","Ep 34: Loss 0.3460\n","Ep 35: Loss 0.3446\n","Ep 36: Loss 0.3146\n","Ep 37: Loss 0.3353\n","Ep 38: Loss 0.3122\n","Ep 39: Loss 0.3343\n","Ep 40: Loss 0.3170\n","Ep 41: Loss 0.3124\n","Ep 42: Loss 0.3288\n","Ep 43: Loss 0.3065\n","Ep 44: Loss 0.3087\n","Ep 45: Loss 0.3010\n","Ep 46: Loss 0.3092\n","Ep 47: Loss 0.3119\n","Ep 48: Loss 0.3046\n","Ep 49: Loss 0.3096\n","Ep 50: Loss 0.3209\n","Finalizing SWA...\n","\n","--- Final Evaluation ---\n","\n","Overall:\n","  accuracy: 0.7735\n","  f1_score: 0.7404\n","  sensitivity: 0.7283\n","  specificity: 0.8095\n","  roc_auc: 0.8549\n","\n","Male:\n","  accuracy: 0.7623\n","  f1_score: 0.7230\n","  sensitivity: 0.7125\n","  specificity: 0.8006\n","  roc_auc: 0.8463\n","\n","Female:\n","  accuracy: 0.7778\n","  f1_score: 0.7469\n","  sensitivity: 0.7342\n","  specificity: 0.8130\n","  roc_auc: 0.8577\n","\n","White:\n","  accuracy: 0.7812\n","  f1_score: 0.7308\n","  sensitivity: 0.7374\n","  specificity: 0.8108\n","  roc_auc: 0.8571\n","\n","Black:\n","  accuracy: 0.7209\n","  f1_score: 0.7586\n","  sensitivity: 0.6875\n","  specificity: 0.7798\n","  roc_auc: 0.8228\n","\n","Asian:\n","  accuracy: 0.7966\n","  f1_score: 0.7750\n","  sensitivity: 0.7561\n","  specificity: 0.8316\n","  roc_auc: 0.8668\n"]}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"Ajz_ZCD2xpW3"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"vinm8RwHxpeX"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"mXfINiepxpoS"},"outputs":[],"execution_count":null}]}